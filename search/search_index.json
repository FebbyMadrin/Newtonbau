{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . This documentation is based on Jetson Nano Development kit","title":"Home"},{"location":"#welcome-to-febby-madrins-documentation","text":"For full documentation visit febbymadrin.github.io . This documentation is based on Jetson Nano Development kit","title":"Welcome to Febby Madrin's documentation"},{"location":"CUDA%20Programming/","text":"tztztgh CUDA Programming Environment used: C++, Pycuda Cross-Compiling, Host: 1 2 3 4 5 #include <stdion.h> int main (){ printf ( \"Hello Febby\" ); return 0 ; } Target: setting up the environment Simple how to install Numba you can try also on https://github.com/jefflgaol/Install-Packages-Jetson-ARM-Family sdsds sdsd how to monitoring the usage of jetson nano https://github.com/rbonghi/jetson_stats https://www.reddit.com/r/JetsonNano/comments/cheqwl/anything_better_than_tegrastats_or_jetsonstats/ https://www.youtube.com/watch?v=7HyKqm6DqnI","title":"CUDA"},{"location":"CUDA%20Programming/#tztztgh","text":"","title":"tztztgh"},{"location":"CUDA%20Programming/#cuda-programming","text":"Environment used: C++, Pycuda Cross-Compiling, Host: 1 2 3 4 5 #include <stdion.h> int main (){ printf ( \"Hello Febby\" ); return 0 ; } Target: setting up the environment","title":"CUDA Programming"},{"location":"CUDA%20Programming/#simple","text":"","title":"Simple"},{"location":"CUDA%20Programming/#how-to-install-numba","text":"you can try also on https://github.com/jefflgaol/Install-Packages-Jetson-ARM-Family sdsds sdsd how to monitoring the usage of jetson nano https://github.com/rbonghi/jetson_stats https://www.reddit.com/r/JetsonNano/comments/cheqwl/anything_better_than_tegrastats_or_jetsonstats/ https://www.youtube.com/watch?v=7HyKqm6DqnI","title":"how to install Numba"},{"location":"CV/","text":"Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"CV"},{"location":"CV/#welcome-to-febby-madrins-documentation","text":"For full documentation visit febbymadrin.github.io .","title":"Welcome to Febby Madrin's documentation"},{"location":"CV/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"CV/#commands_1","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout_1","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"CV/#commands_2","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout_2","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"CV/#commands_3","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout_3","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"CV/#commands_4","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout_4","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"CV/#commands_5","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout_5","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"CV/#commands_6","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout_6","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"CV/#commands_7","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout_7","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"CV/#commands_8","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"CV/#project-layout_8","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"NVIDIA%20Courses/","text":"Newtonbau : Jetson Nano _ Getting Started The following tutorial is a combination of courses organized by NVIDIA, to more detail, you can follow these courses, (at the time of writing these courses are still free), in this section I will summarize 2 couurses at once, who knows in the future there will be changes and not free again, also to simplify and speed up the learning process edited: There are two version of free course 'Getting Started with Jetson Nano', in this tutorial i will summarize the version.1 and startin from 5th Oktober 2020, the second version is available trough this link. version 1 will be not available after 5th April 2021 https://courses.nvidia.com/courses/course-v1:DLI+C-RX-02+V1/info introduction Installation Please download the image for Jetson Nano from the following link Prerequisites In order to be successful in this tutorial, you will need the following: Hardware Jetson Nano Developer Kit Computer with Internet Access and SD card port microSD Memory Card (32GB UHS-I minimum) compatible 5V 4A Power Supply with 2.1mm DC barrel connector 2-pin jumper USB cable (Micro-B to Type-A) compatible USB webcam camera such as Logitech C270 Webcam The complete hardware kit is available from Sparkfun either with the Jetson Nano included or without the Jetson Nano included. Computer A computer with an internet connection and the ability to flash your microSD card An available USBA port on your computer (you may need an adapter if you only have USBC ports | AI and Deep Learning In this tutorial, you'll build AI projects that can answer simple visual questions: Is my hand showing thumbs-up or thumbs-down? Does my face appear happy or sad? How many fingers am I holding up? Where's my nose? Although these questions are easy for any human child to answer, interpreting images with computer vision requires a complex computer model that can be tuned to find the answer in a number of scenarios. For example, a thumbs-up hand signal may be at various angles and distances from the camera, it may be held before a variety of backgrounds, it could be from a variety of different hands, and so on, but it is still a thumbs-up hand signal. An effective AI model must be able to generalize across these scenarios, and even predict the correct answer with new data. As humans, we generalize what we see based on our experience. In a similar way, we can use a branch of AI called Machine Learning to generalize and classify images based on experience in the form of lots of example data. In particular, we will use deep neural network models, or Deep Learning to recognize relevant patterns in an image dataset, and ultimately match new images to correct answers. If you want to know more, you can check out this article about the differences between Artificial Intelligence, Machine Learning, and Deep Learning. Deep Learning Models A Deep Learning model consists of a neural network with internal parameters, or weights, configured to map inputs to outputs. In Image Classification, the inputs are the pixels from a camera image and the outputs are the possible categories, or classes that the model is trained to recognize. The choices might be 1000 different objects, or only two. Multiple labeled examples must be provided to the model over and over to train it to recognize the images. Once the model is trained, it can be run on live data and provide results in real time. This is called inference. Before training, the model cannot accurately determine the correct class from an image input, because the weights are wrong. Labeled examples of images are iteratively submitted to the network with a learning algorithm. If the network gets the \"wrong\" answer (the label doesn't match), the learning algorithm adjusts the weights a little bit. Over many computationally intensive iterations, the accuracy improves to the point that the model can reliably determine the class for an input image. As you will discover, the data that is input is one of the keys to a good model, i.e. one that generalizes well regardless of the background, angle, or other \"noisy\" aspect of the image presented. Additional passes through the data set, or epochs can also improve the model's performance. | Convolutional Neural Networks (CNNs) Deep learning relies on Convolutional Neural Network (CNN) models to transform images into predicted classifications. A CNN is a class of artificial neural network that uses convolutional layers to filter inputs for useful information, and is the preferred network for image applications Artificial Neural Network An artificial neural network is a biologically inspired computational model that is patterned after the network of neurons present in the human brain. At each layer, the network transforms input data by applying a nonlinear function to a weighted sum of the inputs. The intermediate outputs of one layer, called features, are used as the input into the next layer. The neural network, through repeated transformations, learns multiple layers of nonlinear features (like edges and shapes), which it then combines in a final layer to create a prediction (of more complex objects). Convolutions The convolution operation specific to CNNs combines the input data (feature map) from one layer with a convolution kernel (filter) to form a transformed feature map for the next layer. CNNs for image classification are generally composed of an input layer (the image), a series of hidden layers for feature extraction (the convolutions), and a fully connected output layer (the classification). An input image of a traffic sign is filtered by 4 5x5 convolutional kernels which create 4 feature maps, these feature maps are subsampled by max pooling. The next layer applies 10 5x5 convolutional kernels to these subsampled images and again we pool the feature maps. The final layer is a fully connected layer where all generated features are combined and used in the classifier (essentially logistic regression). Image by Maurice Peemen. As it is trained, the CNN adjusts automatically to find the most relevant features based on its classification requirements. For example, a CNN would filter information about the shape of an object when confronted with a general object recognition task but would extract the color of the bird when faced with a bird recognition task. This is based on the CNN's recognition through training that different classes of objects have different shapes but that different types of birds are more likely to differ in color than in shape. Accelerating CNNs Using GPUs The extensive calculations required for training CNN models and running inference through trained CNN models can be quite large in number, requiring intensive compute resources and time. Deep learning frameworks such as Caffe, TensorFlow, and PyTorch, are optimized to run faster on GPUs. The frameworks take advantage of the parallel processing capabilities of a GPU if it is present, speeding up training and inference tasks. The Jetson Nano includes a 128-core NVIDIA Maxwell GPU. Since it can run the full training frameworks, it is also able to re-train networks with transfer learning, a capability you will use in the projects for this tutorial. Jetson Nano enables you to experiment with deep learning and AI on a low-cost platform. See this article for more details on Jetson Nano performance. | ResNet-18 There are a number of world-class CNN architectures available to application developers for image classification and image regression. PyTorch and other frameworks include access to pretrained models from past winners of the famous Imagenet Large Scale Visual Recognition Challenge (ILSVRC), where researchers compete to correctly classify and detect objects and scenes with computer vision algorithms. In 2015, ResNet swept the awards in image classification, detection, and localization. We'll be using the smallest version of ResNet in our projects: ResNet-18. Residual Networks The Deep Residual Learning for Image Recognition research paper provides insight into why this architecture is effective. ResNet is a residual network, made with building blocks that incorporate \"shortcut connections\" that skip one or more layers. The shortcut output is added to the outputs of the skipped layers. The authors demonstrate that this technique makes the network easier to optimize, and have higher accuracy gains at greatly increased depths. The ResNet architectures presented range from 18-layers deep, all the way to 152-layers deep! For our purposes, the smallest network, ResNet-18 provides a good balance of performance and efficiency sized well for the Jetson Nano. Transfer Learning PyTorch includes a pre-trained ResNet-18 model that was trained on the ImageNet 2012 classification dataset, which consists of 1000 classes. In other words, the model can recognize 1000 different objects already! Within the trained neural network are layers that find outlines, curves, lines, and other identifying features of an image. Important image features that were already learned in the original training of the model are now re-usable for our own classification task. We will adapt it for our projects, which all include less than 10 different classes, by modifying the last neural network layer of the 18 that make up the ResNet-18 model. The last layer for ResNet-18 is a fully connected (fc) layer, pooled and flattened to 512 inputs, each connected to the 1000 possible output classes. We will replace the (512,1000) layer with one matching our classes. If we only need three classes, for example, this final layer will become (512, 3), where each of the 512 inputs is fully connected to each one of the 3 output classes. You will still need to train the network to recognize those three classes using images you collect, but since the network has already learned to recognize features common to most objects, training is already part-way done. The previous training can be reused, or \"transferred\" to your new projects. | Thumbs Project The goal of this exercise is to build an Image Classification project that can determine the meaning of hand signals ( thumbs-up or thumbs-down) that are held in front of a live camera. Interactive Tool Startup Steps You will implement the project by collecting your own data, training a model to classify your data, and then testing and updating your model as needed until it correctly classifies thumbs-up or thumbs-down images before the live camera. Step 1: Open The Notebook To get started, navigate to the classification folder in your JupyterLab interface and double-click the classification_interactive.ipynb notebook to open it. Step 2: Execute All Of The Code Blocks The notebook is designed to be reusable for any classification task you wish to build. Step through the code blocks and execute them one at a time. If you have trouble with this step, review the information on JupyterLab. Camera This block sets the size of the images and starts the camera. If your camera is already active in this notebook or in another notebook, first shut down the kernel in the active notebook before running this code cell. Make sure that the correct camera type is selected for execution (USB). This cell may take several seconds to execute. Task You get to define your TASK and CATEGORIES (the classes) parameters here, as well as how many datasets you want to track. For the Thumbs Project, this has already been defined for you, so go ahead and execute the cell. Subdirectories for each class are created to store the example images you collect. The subdirectory names serve as the labels needed for the model. This cell should only take a few seconds to execute. Data Collection You'll collect images for your categories with your camera using an iPython widget. This cell sets up the collection mechanism to count your images and produce the user interface. The widget built here is the data_collection_widget. If you want to learn more about these powerful tools, visit the ipywidgets documentaion. This cell should only take a few seconds to execute. Model This block is where the neural network is defined. First, the GPU device is chosen with the statement: device = torch.device('cuda') The model is set to the ResNet-18 model for this project. Note that the pretrained=True parameter indicates we are loading all the parameter weights for the trained Resnet-18 model, not just the neural network alone: 1 model = torchvision.models.resnet18(pretrained=True) There are a few more models listed in comments that you can try out later if you wish. For more information on available PyTorch pre-trained models, see the PyTorch documentation. In addition to choosing the model, the last layer of the model is modified to accept only the number of classes that we are training for. In the case of the Thumbs Project, it is only 2 (i.e. thumbs-up and thumbs-down). 1 model.fc = torch.nn.Linear(512, len(dataset.categories)) This code cell may take several seconds to execute. Live Execution This code block sets up threading to run the model in the background so that you can view the live camera feed and visualize the model performance in real time. It also includes the code that defines how the outputs from the neural network are categorized. The network produces some value for each of the possible categories. The softmax function takes this vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities. The values now add up to 1 and can be interpreted as probabilities. output = F.softmax(output, dim=1).detach().cpu().numpy().flatten() This cell should only take a few seconds to execute. Training and Evaluation The training code cell sets the hyper-parameters for the model training (number of epochs, batch size, learning rate, momentum) and loads the images for training or evaluation. The model determines a predicted output from the loaded input image. The difference between the predicted output and the actual label is used to calculate the \"loss\". If the model is in training mode, the loss is backpropagated into the network to improve the model. The widgets created by this code cell include the option for setting the number of epochs to run. One epoch is a complete cycle of all images through the trainer. This code cell may take several seconds to execute. Display the Interactive Tool! This is the last code cell. All that's left to do is pack all the widgets into one comprehensive tool and display it. This cell may take several seconds to run and should display the full tool for you to work with. This tool will look essentially the same, no matter how you set up the classification problem with this notebook. Step 3: Collect Your Initial Data The tool is designed for live interaction, so you can collect some data, train it, check the results, and then improve the model with more data and training. We'll try this in pieces to learn what effect the data you gather has on performance of the model. At each step, you'll vary the data in a new way, building your dataset as you go. Collect 30 images of thumbs-up images. Move your thumb through an arc of generally upward angles in front of the camera as you click the \"add\" button to save the data images. Next, select the thumbs-down category on the tool and collect 30 images of your thumb in the down position, again varying the angle a bit as you click. The goal is to provide the model with lots of different examples from each category, so that the prediction can be generalized. Step 4: Train Your Initial Data Set the epoch number to 10, and click the train button. There will be a delay of about 30 seconds as the trainer loads the data. After that, the progress bar will indicate training status for each epoch. You'll see the calculated loss and accuracy displayed as well. With each epoch, the model improves, at least based on the data it has to work with! The accuracy should generally increase. Keep in mind that the accuracy is based on tests against the data the model already has access to, not truly unseen data. Step 5: Test Your Data In Real Time Once training is done, hold your thumb up or down in front of the camera and observe the prediction and sliders. The sliders indicate the probability the model gives for the prediction made. How was the result? Are you satisfied that the model is robust? Try moving the camera to a new background to see if it still works the same. Note : If at any time your camera seems to \"freeze\", it will be necessary to shut down the kernel from the menu bar, then restart the kernel and run all cells. Your data is saved, but the model training will need to be run again Step 6: Improve Your Model Using a different background, gather an additional 30 images for thumbs-up and thumbs-down, again varying the angle. Train an additional 5 epochs. Did your model become more reliable? What happens when you move the thumb to corners and edges of the camera view, or move your thumb very far away or very close to the camera? Using a variety of distances from the camera, gather an additional 30 images for thumbs-up and thumbs-down. Train an additional 5 epochs. Keep testing and training in this way until you are satisfied with the performance of your first project! Step 7: Save Your Model When you are satisfied with your model, save it by entering a name in the \"model path\" box and click \"save model\". | Emotions Project The goal of this exercise is to build an Image Classification project that can determine the meaning of four different facial expressions (\"happy\", \"sad\", \"angry\", \"none\"), that you provide in front of a live camera. Interactive Tool Startup Steps The setup for the Emotions Project is almost the as for the Thumbs Project. Step 1: Open The Notebook You'll use the same classification_interactive.ipynb notebook. If it's already open, restart the notebook and clear all the outputs using the Kernel menu with Kernel->Restart Kernel and Clear All Outputs . If your camera is active in any other notebook, shut down the kernel in that active notebook as well. Step 2: Modify The Task Code Cell Before you execute all of the code blocks in the notebook, you'll need to change the TASK and CATEGORIES parameters in the Task code cell block to define the new project. Comment out the \"thumbs\" project parameters, and uncomment the \"emotions\" parameters: 1 2 3 4 5 6 7 8 9 # TASK = 'thumbs' TASK = 'emotions' # TASK = 'fingers' # TASK = 'diy' # CATEGORIES = ['thumbs_up', 'thumbs_down'] CATEGORIES = ['none', 'happy', 'sad', 'angry'] # CATEGORIES = ['1', '2', '3', '4', '5'] # CATEGORIES = [ 'diy_1', 'diy_2', 'diy_3'] Step 3: Execute All Of The Code Blocks The rest of the blocks remain the same. You'll still use the ResNet18 pre-trained model as a base. This time, since there are four items in the CATEGORIES parameter, there will be four different class subdirectories for data and four output probability sliders on the Interactive tool. Step 4: Collect Data, Train, Test Position the camera in front of your face and collect initial data. As you collect each emotion, vary your head position and pose. Try leaning your head left and right, up and down, side to side. As you create your emotion faces, think about the difference between sad and angry. Exaggerate Your expressions to make them distinctive for the initial training, then refine with more subtlety as you as your model improves: Add 20 images of a \"happy\" face with the happy category selected Add 20 images of a \"sad\" face with the sad category selected Add 20 images of an \"angry\" face with the angry category selected Add 20 images of a face with no expression with the none category selected Set the number of epochs to 10 and click the train button Once the training is complete, try different expressions live and observe the prediction Step 5: Improve Your Model As you did in the Thumbs Project, you can improve your model by adding data for scenarios that don't work as well as you like, then retraining. For example: Move the camera so that the face is closer. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training. Move the camera for a different background. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training. Can you get a friend to try your model? Does it work the same? You know the drill\u2026 more data and training! Step 6: Save Your Model When you are satisfied with your model, save it by entering a name in the \"model path\" box and click \"save model\". More Classification Projects To build another project, follow the pattern you did with the Emotions Project. As an example, the Fingers project is provided as an example, but don't let that limit you! To start a new project, save your previous work, modify the TASK and CATEGORIES values, shutdown and restart the notebook, and run all the cells. Then collect, train, and test! Running example on Jetson Nano Running VisionWorks - SFM Sample","title":"Deep Learning Tutorial"},{"location":"NVIDIA%20Courses/#newtonbau-jetson-nano-_-getting-started","text":"The following tutorial is a combination of courses organized by NVIDIA, to more detail, you can follow these courses, (at the time of writing these courses are still free), in this section I will summarize 2 couurses at once, who knows in the future there will be changes and not free again, also to simplify and speed up the learning process edited: There are two version of free course 'Getting Started with Jetson Nano', in this tutorial i will summarize the version.1 and startin from 5th Oktober 2020, the second version is available trough this link. version 1 will be not available after 5th April 2021 https://courses.nvidia.com/courses/course-v1:DLI+C-RX-02+V1/info","title":"Newtonbau : Jetson Nano _ Getting Started"},{"location":"NVIDIA%20Courses/#introduction","text":"","title":"introduction"},{"location":"NVIDIA%20Courses/#installation","text":"Please download the image for Jetson Nano from the following link","title":"Installation"},{"location":"NVIDIA%20Courses/#prerequisites","text":"In order to be successful in this tutorial, you will need the following:","title":"Prerequisites"},{"location":"NVIDIA%20Courses/#hardware","text":"Jetson Nano Developer Kit Computer with Internet Access and SD card port microSD Memory Card (32GB UHS-I minimum) compatible 5V 4A Power Supply with 2.1mm DC barrel connector 2-pin jumper USB cable (Micro-B to Type-A) compatible USB webcam camera such as Logitech C270 Webcam The complete hardware kit is available from Sparkfun either with the Jetson Nano included or without the Jetson Nano included.","title":"Hardware"},{"location":"NVIDIA%20Courses/#computer","text":"A computer with an internet connection and the ability to flash your microSD card An available USBA port on your computer (you may need an adapter if you only have USBC ports","title":"Computer"},{"location":"NVIDIA%20Courses/#_1","text":"","title":"|"},{"location":"NVIDIA%20Courses/#ai-and-deep-learning","text":"In this tutorial, you'll build AI projects that can answer simple visual questions: Is my hand showing thumbs-up or thumbs-down? Does my face appear happy or sad? How many fingers am I holding up? Where's my nose? Although these questions are easy for any human child to answer, interpreting images with computer vision requires a complex computer model that can be tuned to find the answer in a number of scenarios. For example, a thumbs-up hand signal may be at various angles and distances from the camera, it may be held before a variety of backgrounds, it could be from a variety of different hands, and so on, but it is still a thumbs-up hand signal. An effective AI model must be able to generalize across these scenarios, and even predict the correct answer with new data. As humans, we generalize what we see based on our experience. In a similar way, we can use a branch of AI called Machine Learning to generalize and classify images based on experience in the form of lots of example data. In particular, we will use deep neural network models, or Deep Learning to recognize relevant patterns in an image dataset, and ultimately match new images to correct answers. If you want to know more, you can check out this article about the differences between Artificial Intelligence, Machine Learning, and Deep Learning.","title":"AI and Deep Learning"},{"location":"NVIDIA%20Courses/#deep-learning-models","text":"A Deep Learning model consists of a neural network with internal parameters, or weights, configured to map inputs to outputs. In Image Classification, the inputs are the pixels from a camera image and the outputs are the possible categories, or classes that the model is trained to recognize. The choices might be 1000 different objects, or only two. Multiple labeled examples must be provided to the model over and over to train it to recognize the images. Once the model is trained, it can be run on live data and provide results in real time. This is called inference. Before training, the model cannot accurately determine the correct class from an image input, because the weights are wrong. Labeled examples of images are iteratively submitted to the network with a learning algorithm. If the network gets the \"wrong\" answer (the label doesn't match), the learning algorithm adjusts the weights a little bit. Over many computationally intensive iterations, the accuracy improves to the point that the model can reliably determine the class for an input image. As you will discover, the data that is input is one of the keys to a good model, i.e. one that generalizes well regardless of the background, angle, or other \"noisy\" aspect of the image presented. Additional passes through the data set, or epochs can also improve the model's performance.","title":"Deep Learning Models"},{"location":"NVIDIA%20Courses/#_2","text":"","title":"|"},{"location":"NVIDIA%20Courses/#convolutional-neural-networks-cnns","text":"Deep learning relies on Convolutional Neural Network (CNN) models to transform images into predicted classifications. A CNN is a class of artificial neural network that uses convolutional layers to filter inputs for useful information, and is the preferred network for image applications","title":"Convolutional Neural Networks (CNNs)"},{"location":"NVIDIA%20Courses/#artificial-neural-network","text":"An artificial neural network is a biologically inspired computational model that is patterned after the network of neurons present in the human brain. At each layer, the network transforms input data by applying a nonlinear function to a weighted sum of the inputs. The intermediate outputs of one layer, called features, are used as the input into the next layer. The neural network, through repeated transformations, learns multiple layers of nonlinear features (like edges and shapes), which it then combines in a final layer to create a prediction (of more complex objects).","title":"Artificial Neural Network"},{"location":"NVIDIA%20Courses/#convolutions","text":"The convolution operation specific to CNNs combines the input data (feature map) from one layer with a convolution kernel (filter) to form a transformed feature map for the next layer. CNNs for image classification are generally composed of an input layer (the image), a series of hidden layers for feature extraction (the convolutions), and a fully connected output layer (the classification). An input image of a traffic sign is filtered by 4 5x5 convolutional kernels which create 4 feature maps, these feature maps are subsampled by max pooling. The next layer applies 10 5x5 convolutional kernels to these subsampled images and again we pool the feature maps. The final layer is a fully connected layer where all generated features are combined and used in the classifier (essentially logistic regression). Image by Maurice Peemen. As it is trained, the CNN adjusts automatically to find the most relevant features based on its classification requirements. For example, a CNN would filter information about the shape of an object when confronted with a general object recognition task but would extract the color of the bird when faced with a bird recognition task. This is based on the CNN's recognition through training that different classes of objects have different shapes but that different types of birds are more likely to differ in color than in shape.","title":"Convolutions"},{"location":"NVIDIA%20Courses/#accelerating-cnns-using-gpus","text":"The extensive calculations required for training CNN models and running inference through trained CNN models can be quite large in number, requiring intensive compute resources and time. Deep learning frameworks such as Caffe, TensorFlow, and PyTorch, are optimized to run faster on GPUs. The frameworks take advantage of the parallel processing capabilities of a GPU if it is present, speeding up training and inference tasks. The Jetson Nano includes a 128-core NVIDIA Maxwell GPU. Since it can run the full training frameworks, it is also able to re-train networks with transfer learning, a capability you will use in the projects for this tutorial. Jetson Nano enables you to experiment with deep learning and AI on a low-cost platform. See this article for more details on Jetson Nano performance.","title":"Accelerating CNNs Using GPUs"},{"location":"NVIDIA%20Courses/#_3","text":"","title":"|"},{"location":"NVIDIA%20Courses/#resnet-18","text":"There are a number of world-class CNN architectures available to application developers for image classification and image regression. PyTorch and other frameworks include access to pretrained models from past winners of the famous Imagenet Large Scale Visual Recognition Challenge (ILSVRC), where researchers compete to correctly classify and detect objects and scenes with computer vision algorithms. In 2015, ResNet swept the awards in image classification, detection, and localization. We'll be using the smallest version of ResNet in our projects: ResNet-18.","title":"ResNet-18"},{"location":"NVIDIA%20Courses/#residual-networks","text":"The Deep Residual Learning for Image Recognition research paper provides insight into why this architecture is effective. ResNet is a residual network, made with building blocks that incorporate \"shortcut connections\" that skip one or more layers. The shortcut output is added to the outputs of the skipped layers. The authors demonstrate that this technique makes the network easier to optimize, and have higher accuracy gains at greatly increased depths. The ResNet architectures presented range from 18-layers deep, all the way to 152-layers deep! For our purposes, the smallest network, ResNet-18 provides a good balance of performance and efficiency sized well for the Jetson Nano.","title":"Residual Networks"},{"location":"NVIDIA%20Courses/#transfer-learning","text":"PyTorch includes a pre-trained ResNet-18 model that was trained on the ImageNet 2012 classification dataset, which consists of 1000 classes. In other words, the model can recognize 1000 different objects already! Within the trained neural network are layers that find outlines, curves, lines, and other identifying features of an image. Important image features that were already learned in the original training of the model are now re-usable for our own classification task. We will adapt it for our projects, which all include less than 10 different classes, by modifying the last neural network layer of the 18 that make up the ResNet-18 model. The last layer for ResNet-18 is a fully connected (fc) layer, pooled and flattened to 512 inputs, each connected to the 1000 possible output classes. We will replace the (512,1000) layer with one matching our classes. If we only need three classes, for example, this final layer will become (512, 3), where each of the 512 inputs is fully connected to each one of the 3 output classes. You will still need to train the network to recognize those three classes using images you collect, but since the network has already learned to recognize features common to most objects, training is already part-way done. The previous training can be reused, or \"transferred\" to your new projects.","title":"Transfer Learning"},{"location":"NVIDIA%20Courses/#_4","text":"","title":"|"},{"location":"NVIDIA%20Courses/#thumbs-project","text":"The goal of this exercise is to build an Image Classification project that can determine the meaning of hand signals ( thumbs-up or thumbs-down) that are held in front of a live camera.","title":"Thumbs Project"},{"location":"NVIDIA%20Courses/#interactive-tool-startup-steps","text":"You will implement the project by collecting your own data, training a model to classify your data, and then testing and updating your model as needed until it correctly classifies thumbs-up or thumbs-down images before the live camera.","title":"Interactive Tool Startup Steps"},{"location":"NVIDIA%20Courses/#step-1-open-the-notebook","text":"To get started, navigate to the classification folder in your JupyterLab interface and double-click the classification_interactive.ipynb notebook to open it.","title":"Step 1: Open The Notebook"},{"location":"NVIDIA%20Courses/#step-2-execute-all-of-the-code-blocks","text":"The notebook is designed to be reusable for any classification task you wish to build. Step through the code blocks and execute them one at a time. If you have trouble with this step, review the information on JupyterLab. Camera This block sets the size of the images and starts the camera. If your camera is already active in this notebook or in another notebook, first shut down the kernel in the active notebook before running this code cell. Make sure that the correct camera type is selected for execution (USB). This cell may take several seconds to execute. Task You get to define your TASK and CATEGORIES (the classes) parameters here, as well as how many datasets you want to track. For the Thumbs Project, this has already been defined for you, so go ahead and execute the cell. Subdirectories for each class are created to store the example images you collect. The subdirectory names serve as the labels needed for the model. This cell should only take a few seconds to execute. Data Collection You'll collect images for your categories with your camera using an iPython widget. This cell sets up the collection mechanism to count your images and produce the user interface. The widget built here is the data_collection_widget. If you want to learn more about these powerful tools, visit the ipywidgets documentaion. This cell should only take a few seconds to execute. Model This block is where the neural network is defined. First, the GPU device is chosen with the statement: device = torch.device('cuda') The model is set to the ResNet-18 model for this project. Note that the pretrained=True parameter indicates we are loading all the parameter weights for the trained Resnet-18 model, not just the neural network alone: 1 model = torchvision.models.resnet18(pretrained=True) There are a few more models listed in comments that you can try out later if you wish. For more information on available PyTorch pre-trained models, see the PyTorch documentation. In addition to choosing the model, the last layer of the model is modified to accept only the number of classes that we are training for. In the case of the Thumbs Project, it is only 2 (i.e. thumbs-up and thumbs-down). 1 model.fc = torch.nn.Linear(512, len(dataset.categories)) This code cell may take several seconds to execute. Live Execution This code block sets up threading to run the model in the background so that you can view the live camera feed and visualize the model performance in real time. It also includes the code that defines how the outputs from the neural network are categorized. The network produces some value for each of the possible categories. The softmax function takes this vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities. The values now add up to 1 and can be interpreted as probabilities. output = F.softmax(output, dim=1).detach().cpu().numpy().flatten() This cell should only take a few seconds to execute. Training and Evaluation The training code cell sets the hyper-parameters for the model training (number of epochs, batch size, learning rate, momentum) and loads the images for training or evaluation. The model determines a predicted output from the loaded input image. The difference between the predicted output and the actual label is used to calculate the \"loss\". If the model is in training mode, the loss is backpropagated into the network to improve the model. The widgets created by this code cell include the option for setting the number of epochs to run. One epoch is a complete cycle of all images through the trainer. This code cell may take several seconds to execute. Display the Interactive Tool! This is the last code cell. All that's left to do is pack all the widgets into one comprehensive tool and display it. This cell may take several seconds to run and should display the full tool for you to work with. This tool will look essentially the same, no matter how you set up the classification problem with this notebook.","title":"Step 2: Execute All Of The Code Blocks"},{"location":"NVIDIA%20Courses/#step-3-collect-your-initial-data","text":"The tool is designed for live interaction, so you can collect some data, train it, check the results, and then improve the model with more data and training. We'll try this in pieces to learn what effect the data you gather has on performance of the model. At each step, you'll vary the data in a new way, building your dataset as you go. Collect 30 images of thumbs-up images. Move your thumb through an arc of generally upward angles in front of the camera as you click the \"add\" button to save the data images. Next, select the thumbs-down category on the tool and collect 30 images of your thumb in the down position, again varying the angle a bit as you click. The goal is to provide the model with lots of different examples from each category, so that the prediction can be generalized.","title":"Step 3: Collect Your Initial Data"},{"location":"NVIDIA%20Courses/#step-4-train-your-initial-data","text":"Set the epoch number to 10, and click the train button. There will be a delay of about 30 seconds as the trainer loads the data. After that, the progress bar will indicate training status for each epoch. You'll see the calculated loss and accuracy displayed as well. With each epoch, the model improves, at least based on the data it has to work with! The accuracy should generally increase. Keep in mind that the accuracy is based on tests against the data the model already has access to, not truly unseen data.","title":"Step 4: Train Your Initial Data"},{"location":"NVIDIA%20Courses/#step-5-test-your-data-in-real-time","text":"Once training is done, hold your thumb up or down in front of the camera and observe the prediction and sliders. The sliders indicate the probability the model gives for the prediction made. How was the result? Are you satisfied that the model is robust? Try moving the camera to a new background to see if it still works the same. Note : If at any time your camera seems to \"freeze\", it will be necessary to shut down the kernel from the menu bar, then restart the kernel and run all cells. Your data is saved, but the model training will need to be run again","title":"Step 5: Test Your Data In Real Time"},{"location":"NVIDIA%20Courses/#step-6-improve-your-model","text":"Using a different background, gather an additional 30 images for thumbs-up and thumbs-down, again varying the angle. Train an additional 5 epochs. Did your model become more reliable? What happens when you move the thumb to corners and edges of the camera view, or move your thumb very far away or very close to the camera? Using a variety of distances from the camera, gather an additional 30 images for thumbs-up and thumbs-down. Train an additional 5 epochs. Keep testing and training in this way until you are satisfied with the performance of your first project!","title":"Step 6: Improve Your Model"},{"location":"NVIDIA%20Courses/#step-7-save-your-model","text":"When you are satisfied with your model, save it by entering a name in the \"model path\" box and click \"save model\".","title":"Step 7: Save Your Model"},{"location":"NVIDIA%20Courses/#_5","text":"","title":"|"},{"location":"NVIDIA%20Courses/#emotions-project","text":"The goal of this exercise is to build an Image Classification project that can determine the meaning of four different facial expressions (\"happy\", \"sad\", \"angry\", \"none\"), that you provide in front of a live camera.","title":"Emotions Project"},{"location":"NVIDIA%20Courses/#interactive-tool-startup-steps_1","text":"The setup for the Emotions Project is almost the as for the Thumbs Project.","title":"Interactive Tool Startup Steps"},{"location":"NVIDIA%20Courses/#step-1-open-the-notebook_1","text":"You'll use the same classification_interactive.ipynb notebook. If it's already open, restart the notebook and clear all the outputs using the Kernel menu with Kernel->Restart Kernel and Clear All Outputs . If your camera is active in any other notebook, shut down the kernel in that active notebook as well.","title":"Step 1: Open The Notebook"},{"location":"NVIDIA%20Courses/#step-2-modify-the-task-code-cell","text":"Before you execute all of the code blocks in the notebook, you'll need to change the TASK and CATEGORIES parameters in the Task code cell block to define the new project. Comment out the \"thumbs\" project parameters, and uncomment the \"emotions\" parameters: 1 2 3 4 5 6 7 8 9 # TASK = 'thumbs' TASK = 'emotions' # TASK = 'fingers' # TASK = 'diy' # CATEGORIES = ['thumbs_up', 'thumbs_down'] CATEGORIES = ['none', 'happy', 'sad', 'angry'] # CATEGORIES = ['1', '2', '3', '4', '5'] # CATEGORIES = [ 'diy_1', 'diy_2', 'diy_3']","title":"Step 2: Modify The Task Code Cell"},{"location":"NVIDIA%20Courses/#step-3-execute-all-of-the-code-blocks","text":"The rest of the blocks remain the same. You'll still use the ResNet18 pre-trained model as a base. This time, since there are four items in the CATEGORIES parameter, there will be four different class subdirectories for data and four output probability sliders on the Interactive tool.","title":"Step 3: Execute All Of The Code Blocks"},{"location":"NVIDIA%20Courses/#step-4-collect-data-train-test","text":"Position the camera in front of your face and collect initial data. As you collect each emotion, vary your head position and pose. Try leaning your head left and right, up and down, side to side. As you create your emotion faces, think about the difference between sad and angry. Exaggerate Your expressions to make them distinctive for the initial training, then refine with more subtlety as you as your model improves: Add 20 images of a \"happy\" face with the happy category selected Add 20 images of a \"sad\" face with the sad category selected Add 20 images of an \"angry\" face with the angry category selected Add 20 images of a face with no expression with the none category selected Set the number of epochs to 10 and click the train button Once the training is complete, try different expressions live and observe the prediction","title":"Step 4: Collect Data, Train, Test"},{"location":"NVIDIA%20Courses/#step-5-improve-your-model","text":"As you did in the Thumbs Project, you can improve your model by adding data for scenarios that don't work as well as you like, then retraining. For example: Move the camera so that the face is closer. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training. Move the camera for a different background. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training. Can you get a friend to try your model? Does it work the same? You know the drill\u2026 more data and training!","title":"Step 5: Improve Your Model"},{"location":"NVIDIA%20Courses/#step-6-save-your-model","text":"When you are satisfied with your model, save it by entering a name in the \"model path\" box and click \"save model\".","title":"Step 6: Save Your Model"},{"location":"NVIDIA%20Courses/#more-classification-projects","text":"To build another project, follow the pattern you did with the Emotions Project. As an example, the Fingers project is provided as an example, but don't let that limit you! To start a new project, save your previous work, modify the TASK and CATEGORIES values, shutdown and restart the notebook, and run all the cells. Then collect, train, and test!","title":"More Classification Projects"},{"location":"NVIDIA%20Courses/#running-example-on-jetson-nano","text":"","title":"Running example on Jetson Nano"},{"location":"NVIDIA%20Courses/#running-visionworks-sfm-sample","text":"","title":"Running VisionWorks - SFM Sample"},{"location":"Niblack%20Binarization/","text":"What is binarization Image binarization is the process of taking a grayscale image and converting it to black-and-white, essentially reducing the information contained within the image from 256 shades of gray to 2: black and white, a binary image. This is sometimes known as image thresholding , although thresholding may produce images with more than 2 levels of gray. It is a form or segmentation, whereby an image is divided into constituent objects. This is a task commonly performed when trying to extract an object from an image. However like many image processing operations, it is not trivial, and is solely dependent on the content within the image. The trick is images that may seem easy to convert to B&W are many times not. a very good explanation can go trough this website: https://craftofcoding.wordpress.com : https://felixniklas.com/imageprocessing/binarization : Image Binarization (Simple) C++ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include \"opencv2/imgproc.hpp\" #include \"opencv2/imgcodecs.hpp\" #include \"opencv2/highgui.hpp\" #include <iostream> using namespace cv ; using std :: cout ; int threshold_value = 0 ; int threshold_type = 3 ; int const max_value = 255 ; int const max_type = 4 ; int const max_binary_value = 255 ; Mat src , src_gray , dst ; const char * window_name = \"Threshold Demo\" ; const char * trackbar_type = \"Type: \\n 0: Binary \\n 1: Binary Inverted \\n 2: Truncate \\n 3: To Zero \\n 4: To Zero Inverted\" ; const char * trackbar_value = \"Value\" ; static void Threshold_Demo ( int , void * ) { /* 0: Binary 1: Binary Inverted 2: Threshold Truncated 3: Threshold to Zero 4: Threshold to Zero Inverted */ threshold ( src_gray , dst , threshold_value , max_binary_value , threshold_type ); imshow ( window_name , dst ); } int main ( int argc , char ** argv ) { String imageName ( \"stuff.jpg\" ); // by default if ( argc > 1 ) { imageName = argv [ 1 ]; } src = imread ( samples :: findFile ( imageName ), IMREAD_COLOR ); // Load an image if ( src . empty ()) { cout << \"Cannot read the image: \" << imageName << std :: endl ; return -1 ; } cvtColor ( src , src_gray , COLOR_BGR2GRAY ); // Convert the image to Gray namedWindow ( window_name , WINDOW_AUTOSIZE ); // Create a window to display results createTrackbar ( trackbar_type , window_name , & threshold_type , max_type , Threshold_Demo ); // Create a Trackbar to choose type of Threshold createTrackbar ( trackbar_value , window_name , & threshold_value , max_value , Threshold_Demo ); // Create a Trackbar to choose Threshold value Threshold_Demo ( 0 , 0 ); // Call the function to initialize waitKey (); return 0 ; } Video Binarization (Simple) C++ Python /home/dlinano/Downloads 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import cv2 def main (): windowName = [ 'Binary' , 'Binary Inv' , 'Zero' , 'Zero Inv' , 'Trunc' ] cap = cv2 . VideoCapture ( 0 ) if cap . isOpened (): ret , frame = cap . read () else : ret = False while ret : ret , frame = cap . read () th = 178 max_val = 255 ret , o1 = cv2 . threshold ( frame , th , max_val , cv2 . THRESH_BINARY ) # ret, o2 = cv2.threshold(frame, th, max_val, cv2.THRESH_BINARY_INV) # ret, o3 = cv2.threshold(frame, th, max_val, cv2.THRESH_TOZERO) # ret, o4 = cv2.threshold(frame, th, max_val, cv2.THRESH_TOZERO_INV) # ret, o5 = cv2.threshold(frame, th, max_val, cv2.THRESH_TRUNC) cv2 . imshow ( windowName [ 0 ], o1 ) # cv2.imshow(windowName[1], o2) # cv2.imshow(windowName[2], o3) # cv2.imshow(windowName[3], o4) # cv2.imshow(windowName[4], o5) if cv2 . waitKey ( 1 ) & 0xFF == ord ( 'q' ): break cv2 . destroyAllWindows () cap . release () if __name__ == \"__main__\" : main () Image Binarization (NiBlack mode) Video Binarization (NiBlack mode) - Febby The Comparison between the performance of Jetson Nano when not using CUDA and when using CUDA with CUDA VS no CUDA CPU Usage programs that do not use CUDA, take more power/usage from the CPU (for programs running video / not streaming) however, programs that are streaming, using CUDA or not, do not have a significant difference GPU Usage programs that use CUDA will use more GPU power than without CUDA (for programs running video file/ not streaming) however, programs that are streaming, using CUDA or not, do not have a significant difference Temperature Level The change in temperature is not too big / significant Current Usage on CPU programs that uses CUDA, it can be seen that it is more current-efficient, but this can only happen when the program is running a video file on the contrary, when running a streaming application, the difference cannot be seen Current Usage on GPU programs that use CUDA, consume more current, but not so much, not as much current as CPU usage Summary programs that use CUDA, will improve the performance of jerson nano, this can be viewed from several aspects as follows: output image clarity, as it can produce a clearer image and has less noise. CPU usage, programs with CUDA also have higher effectiveness, this can be proven when by running the same task, the CPU percentage is lowerr speed, with CUDA we can run video files faster and smoother current consumption, the program with CUDA is also has better performance, because it can significantly reduce current consumption up to 200mA - 300mA","title":"Niblack Binarization"},{"location":"Niblack%20Binarization/#what-is-binarization","text":"Image binarization is the process of taking a grayscale image and converting it to black-and-white, essentially reducing the information contained within the image from 256 shades of gray to 2: black and white, a binary image. This is sometimes known as image thresholding , although thresholding may produce images with more than 2 levels of gray. It is a form or segmentation, whereby an image is divided into constituent objects. This is a task commonly performed when trying to extract an object from an image. However like many image processing operations, it is not trivial, and is solely dependent on the content within the image. The trick is images that may seem easy to convert to B&W are many times not. a very good explanation can go trough this website: https://craftofcoding.wordpress.com : https://felixniklas.com/imageprocessing/binarization :","title":"What is binarization"},{"location":"Niblack%20Binarization/#image-binarization-simple","text":"C++ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 #include \"opencv2/imgproc.hpp\" #include \"opencv2/imgcodecs.hpp\" #include \"opencv2/highgui.hpp\" #include <iostream> using namespace cv ; using std :: cout ; int threshold_value = 0 ; int threshold_type = 3 ; int const max_value = 255 ; int const max_type = 4 ; int const max_binary_value = 255 ; Mat src , src_gray , dst ; const char * window_name = \"Threshold Demo\" ; const char * trackbar_type = \"Type: \\n 0: Binary \\n 1: Binary Inverted \\n 2: Truncate \\n 3: To Zero \\n 4: To Zero Inverted\" ; const char * trackbar_value = \"Value\" ; static void Threshold_Demo ( int , void * ) { /* 0: Binary 1: Binary Inverted 2: Threshold Truncated 3: Threshold to Zero 4: Threshold to Zero Inverted */ threshold ( src_gray , dst , threshold_value , max_binary_value , threshold_type ); imshow ( window_name , dst ); } int main ( int argc , char ** argv ) { String imageName ( \"stuff.jpg\" ); // by default if ( argc > 1 ) { imageName = argv [ 1 ]; } src = imread ( samples :: findFile ( imageName ), IMREAD_COLOR ); // Load an image if ( src . empty ()) { cout << \"Cannot read the image: \" << imageName << std :: endl ; return -1 ; } cvtColor ( src , src_gray , COLOR_BGR2GRAY ); // Convert the image to Gray namedWindow ( window_name , WINDOW_AUTOSIZE ); // Create a window to display results createTrackbar ( trackbar_type , window_name , & threshold_type , max_type , Threshold_Demo ); // Create a Trackbar to choose type of Threshold createTrackbar ( trackbar_value , window_name , & threshold_value , max_value , Threshold_Demo ); // Create a Trackbar to choose Threshold value Threshold_Demo ( 0 , 0 ); // Call the function to initialize waitKey (); return 0 ; }","title":"Image Binarization (Simple)"},{"location":"Niblack%20Binarization/#video-binarization-simple","text":"","title":"Video Binarization (Simple)"},{"location":"Niblack%20Binarization/#c","text":"","title":"C++"},{"location":"Niblack%20Binarization/#python","text":"/home/dlinano/Downloads 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import cv2 def main (): windowName = [ 'Binary' , 'Binary Inv' , 'Zero' , 'Zero Inv' , 'Trunc' ] cap = cv2 . VideoCapture ( 0 ) if cap . isOpened (): ret , frame = cap . read () else : ret = False while ret : ret , frame = cap . read () th = 178 max_val = 255 ret , o1 = cv2 . threshold ( frame , th , max_val , cv2 . THRESH_BINARY ) # ret, o2 = cv2.threshold(frame, th, max_val, cv2.THRESH_BINARY_INV) # ret, o3 = cv2.threshold(frame, th, max_val, cv2.THRESH_TOZERO) # ret, o4 = cv2.threshold(frame, th, max_val, cv2.THRESH_TOZERO_INV) # ret, o5 = cv2.threshold(frame, th, max_val, cv2.THRESH_TRUNC) cv2 . imshow ( windowName [ 0 ], o1 ) # cv2.imshow(windowName[1], o2) # cv2.imshow(windowName[2], o3) # cv2.imshow(windowName[3], o4) # cv2.imshow(windowName[4], o5) if cv2 . waitKey ( 1 ) & 0xFF == ord ( 'q' ): break cv2 . destroyAllWindows () cap . release () if __name__ == \"__main__\" : main ()","title":"Python"},{"location":"Niblack%20Binarization/#image-binarization-niblack-mode","text":"","title":"Image Binarization (NiBlack mode)"},{"location":"Niblack%20Binarization/#video-binarization-niblack-mode-febby","text":"","title":"Video Binarization (NiBlack mode) - Febby"},{"location":"Niblack%20Binarization/#the-comparison-between-the-performance-of-jetson-nano-when-not-using-cuda-and-when-using-cuda","text":"with CUDA VS no CUDA CPU Usage programs that do not use CUDA, take more power/usage from the CPU (for programs running video / not streaming) however, programs that are streaming, using CUDA or not, do not have a significant difference GPU Usage programs that use CUDA will use more GPU power than without CUDA (for programs running video file/ not streaming) however, programs that are streaming, using CUDA or not, do not have a significant difference Temperature Level The change in temperature is not too big / significant Current Usage on CPU programs that uses CUDA, it can be seen that it is more current-efficient, but this can only happen when the program is running a video file on the contrary, when running a streaming application, the difference cannot be seen Current Usage on GPU programs that use CUDA, consume more current, but not so much, not as much current as CPU usage","title":"The Comparison between the performance of Jetson Nano when not using CUDA and when using CUDA"},{"location":"Niblack%20Binarization/#summary","text":"programs that use CUDA, will improve the performance of jerson nano, this can be viewed from several aspects as follows: output image clarity, as it can produce a clearer image and has less noise. CPU usage, programs with CUDA also have higher effectiveness, this can be proven when by running the same task, the CPU percentage is lowerr speed, with CUDA we can run video files faster and smoother current consumption, the program with CUDA is also has better performance, because it can significantly reduce current consumption up to 200mA - 300mA","title":"Summary"},{"location":"OpenCV%20Programming/","text":"The following steps have been tested for Ubuntu 10.04 but should work with other distros as well. Required Packages GCC 4.4.x or later CMake 2.8.7 or higher Git GTK+2.x or higher, including headers (libgtk2.0-dev) pkg-config Python 2.6 or later and Numpy 1.5 or later with developer packages (python-dev, python-numpy) ffmpeg or libav development packages: libavcodec-dev, libavformat-dev, libswscale-dev [optional] libtbb2 libtbb-dev [optional] libdc1394 2.x [optional] libjpeg-dev, libpng-dev, libtiff-dev, libjasper-dev, libdc1394-22-dev [optional] CUDA Toolkit 6.5 or higher The packages can be installed using a terminal and the following commands or by using Synaptic Manager: [compiler] sudo apt-get install build-essential [required] sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev [optional] sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev Getting OpenCV Source Code You can use the latest stable OpenCV version or you can grab the latest snapshot from our Git repository . Getting the Latest Stable OpenCV Version Go to our downloads page . Download the source archive and unpack it. Getting the Cutting-edge OpenCV from the Git Repository Launch Git client and clone OpenCV repository . If you need modules from OpenCV contrib repository then clone it as well. For example cd ~/ git clone https://github.com/opencv/opencv.git git clone https://github.com/opencv/opencv_contrib.git Building OpenCV from Source Using CMake Create a temporary directory, which we denote as , where you want to put the generated Makefiles, project files as well the object files and output binaries and enter there. For example cd ~/opencv mkdir build cd build Configuring. Run cmake [ ] For example cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local .. or cmake-gui set full path to OpenCV source code, e.g. /home/user/opencv set full path to , e.g. /home/user/opencv/build set optional parameters run: ?Configure? run: ?Generate? Note Use cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local .. , without spaces after -D if the above example doesn't work. Description of some parameters build type: CMAKE_BUILD_TYPE=Release\\Debug to build with modules from opencv_contrib set OPENCV_EXTRA_MODULES_PATH to set BUILD_DOCS for building documents set BUILD_EXAMPLES to build all examples [optional] Building python. Set the following python parameters: PYTHON2(3)_EXECUTABLE = PYTHON_INCLUDE_DIR = /usr/include/python PYTHON_INCLUDE_DIR2 = /usr/include/x86_64-linux-gnu/python PYTHON_LIBRARY = /usr/lib/x86_64-linux-gnu/libpython .so PYTHON2(3)_NUMPY_INCLUDE_DIRS = /usr/lib/python /dist-packages/numpy/core/include/ [optional] Building java. Unset parameter: BUILD_SHARED_LIBS It is useful also to unset BUILD_EXAMPLES, BUILD_TESTS, BUILD_PERF_TESTS - as they all will be statically linked with OpenCV and can take a lot of memory. Build. From build directory execute make , it is recommended to do this in several threads For example make -j7 # runs 7 jobs in parallel [optional] Building documents. Enter and run make with target \"doxygen\" For example cd ~/opencv/build/doc/ make -j7 doxygen To install libraries, execute the following command from build directory sudo make install [optional] Running tests Get the required test data from OpenCV extra repository . For example git clone https://github.com/opencv/opencv_extra.git set OPENCV_TEST_DATA_PATH environment variable to . execute tests from build directory. For example /bin/opencv_test_core Note If the size of the created library is a critical issue (like in case of an Android build) you can use the install/strip command to get the smallest size possible. The stripped version appears to be twice as small. However, we do not recommend using this unless those extra megabytes do really matter. How to build OpenCV with extra modules You can build OpenCV, so it will include the modules from this repository. Contrib modules are under constant development and it is recommended to use them alongside the master branch or latest releases of OpenCV. Here is the CMake command for you: 1 2 3 $ cd <opencv_build_directory> $ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules <opencv_source_directory> $ make -j5 As the result, OpenCV will be built in the <opencv_build_directory> with all modules from opencv_contrib repository. If you don't want all of the modules, use CMake's BUILD_opencv_* options. Like in this example: 1 $ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules -DBUILD_opencv_legacy=OFF <opencv_source_directory> If you also want to build the samples from the \"samples\" folder of each module, also include the \"-DBUILD_EXAMPLES=ON\" option. If you prefer using the gui version of cmake (cmake-gui), then, you can add opencv_contrib modules within opencv core by doing the following: start cmake-gui select the opencv source code folder and the folder where binaries will be built (the 2 upper forms of the interface) press the configure button. you will see all the opencv build parameters in the central interface browse the parameters and look for the form called OPENCV_EXTRA_MODULES_PATH (use the search form to focus rapidly on it) complete this OPENCV_EXTRA_MODULES_PATH by the proper pathname to the <opencv_contrib>/modules value using its browse button. press the configure button followed by the generate button (the first time, you will be asked which makefile style to use) build the opencv core with the method you chose (make and make install if you chose Unix makefile at step 6) to run, linker flags to contrib modules will need to be added to use them in your code/IDE. For example to use the aruco module, \"-lopencv_aruco\" flag will be added. Update the repository documentation In order to keep a clean overview containing all contributed modules the following files need to be created/adapted. Update the README.md file under the modules folder. Here you add your model with a single line description. Add a README.md inside your own module folder. This README explains which functionality (separate functions) is available, links to the corresponding samples and explains in somewhat more detail what the module is expected to do. If any extra requirements are needed to build the module without problems, add them here also. daripada memakai CMAke, meiding pakai c++ contohnya pada pemakaaain g++ opencv_version.cpp $(pkg-config --cflags --libs opencv4) -o show_version g++ niblack_thresholding.cpp.o $(pkg-config --cflags --libs opencv4) -o show_version -lopencv_ximgproc cap.set(cv2.CAP_PROP_FPS, 30)","title":"OpenCV"},{"location":"OpenCV%20Programming/#required-packages","text":"GCC 4.4.x or later CMake 2.8.7 or higher Git GTK+2.x or higher, including headers (libgtk2.0-dev) pkg-config Python 2.6 or later and Numpy 1.5 or later with developer packages (python-dev, python-numpy) ffmpeg or libav development packages: libavcodec-dev, libavformat-dev, libswscale-dev [optional] libtbb2 libtbb-dev [optional] libdc1394 2.x [optional] libjpeg-dev, libpng-dev, libtiff-dev, libjasper-dev, libdc1394-22-dev [optional] CUDA Toolkit 6.5 or higher The packages can be installed using a terminal and the following commands or by using Synaptic Manager: [compiler] sudo apt-get install build-essential [required] sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev [optional] sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev","title":"Required Packages"},{"location":"OpenCV%20Programming/#getting-opencv-source-code","text":"You can use the latest stable OpenCV version or you can grab the latest snapshot from our Git repository .","title":"Getting OpenCV Source Code"},{"location":"OpenCV%20Programming/#getting-the-latest-stable-opencv-version","text":"Go to our downloads page . Download the source archive and unpack it.","title":"Getting the Latest Stable OpenCV Version"},{"location":"OpenCV%20Programming/#getting-the-cutting-edge-opencv-from-the-git-repository","text":"Launch Git client and clone OpenCV repository . If you need modules from OpenCV contrib repository then clone it as well. For example cd ~/ git clone https://github.com/opencv/opencv.git git clone https://github.com/opencv/opencv_contrib.git","title":"Getting the Cutting-edge OpenCV from the Git Repository"},{"location":"OpenCV%20Programming/#building-opencv-from-source-using-cmake","text":"Create a temporary directory, which we denote as , where you want to put the generated Makefiles, project files as well the object files and output binaries and enter there. For example cd ~/opencv mkdir build cd build Configuring. Run cmake [ ] For example cmake -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=/usr/local .. or cmake-gui set full path to OpenCV source code, e.g. /home/user/opencv set full path to , e.g. /home/user/opencv/build set optional parameters run: ?Configure? run: ?Generate? Note Use cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local .. , without spaces after -D if the above example doesn't work. Description of some parameters build type: CMAKE_BUILD_TYPE=Release\\Debug to build with modules from opencv_contrib set OPENCV_EXTRA_MODULES_PATH to set BUILD_DOCS for building documents set BUILD_EXAMPLES to build all examples [optional] Building python. Set the following python parameters: PYTHON2(3)_EXECUTABLE = PYTHON_INCLUDE_DIR = /usr/include/python PYTHON_INCLUDE_DIR2 = /usr/include/x86_64-linux-gnu/python PYTHON_LIBRARY = /usr/lib/x86_64-linux-gnu/libpython .so PYTHON2(3)_NUMPY_INCLUDE_DIRS = /usr/lib/python /dist-packages/numpy/core/include/ [optional] Building java. Unset parameter: BUILD_SHARED_LIBS It is useful also to unset BUILD_EXAMPLES, BUILD_TESTS, BUILD_PERF_TESTS - as they all will be statically linked with OpenCV and can take a lot of memory. Build. From build directory execute make , it is recommended to do this in several threads For example make -j7 # runs 7 jobs in parallel [optional] Building documents. Enter and run make with target \"doxygen\" For example cd ~/opencv/build/doc/ make -j7 doxygen To install libraries, execute the following command from build directory sudo make install [optional] Running tests Get the required test data from OpenCV extra repository . For example git clone https://github.com/opencv/opencv_extra.git set OPENCV_TEST_DATA_PATH environment variable to . execute tests from build directory. For example /bin/opencv_test_core Note If the size of the created library is a critical issue (like in case of an Android build) you can use the install/strip command to get the smallest size possible. The stripped version appears to be twice as small. However, we do not recommend using this unless those extra megabytes do really matter.","title":"Building OpenCV from Source Using CMake"},{"location":"OpenCV%20Programming/#how-to-build-opencv-with-extra-modules","text":"You can build OpenCV, so it will include the modules from this repository. Contrib modules are under constant development and it is recommended to use them alongside the master branch or latest releases of OpenCV. Here is the CMake command for you: 1 2 3 $ cd <opencv_build_directory> $ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules <opencv_source_directory> $ make -j5 As the result, OpenCV will be built in the <opencv_build_directory> with all modules from opencv_contrib repository. If you don't want all of the modules, use CMake's BUILD_opencv_* options. Like in this example: 1 $ cmake -DOPENCV_EXTRA_MODULES_PATH=<opencv_contrib>/modules -DBUILD_opencv_legacy=OFF <opencv_source_directory> If you also want to build the samples from the \"samples\" folder of each module, also include the \"-DBUILD_EXAMPLES=ON\" option. If you prefer using the gui version of cmake (cmake-gui), then, you can add opencv_contrib modules within opencv core by doing the following: start cmake-gui select the opencv source code folder and the folder where binaries will be built (the 2 upper forms of the interface) press the configure button. you will see all the opencv build parameters in the central interface browse the parameters and look for the form called OPENCV_EXTRA_MODULES_PATH (use the search form to focus rapidly on it) complete this OPENCV_EXTRA_MODULES_PATH by the proper pathname to the <opencv_contrib>/modules value using its browse button. press the configure button followed by the generate button (the first time, you will be asked which makefile style to use) build the opencv core with the method you chose (make and make install if you chose Unix makefile at step 6) to run, linker flags to contrib modules will need to be added to use them in your code/IDE. For example to use the aruco module, \"-lopencv_aruco\" flag will be added.","title":"How to build OpenCV with extra modules"},{"location":"OpenCV%20Programming/#update-the-repository-documentation","text":"In order to keep a clean overview containing all contributed modules the following files need to be created/adapted. Update the README.md file under the modules folder. Here you add your model with a single line description. Add a README.md inside your own module folder. This README explains which functionality (separate functions) is available, links to the corresponding samples and explains in somewhat more detail what the module is expected to do. If any extra requirements are needed to build the module without problems, add them here also. daripada memakai CMAke, meiding pakai c++ contohnya pada pemakaaain g++ opencv_version.cpp $(pkg-config --cflags --libs opencv4) -o show_version g++ niblack_thresholding.cpp.o $(pkg-config --cflags --libs opencv4) -o show_version -lopencv_ximgproc cap.set(cv2.CAP_PROP_FPS, 30)","title":"Update the repository documentation"},{"location":"Setup%20and%20Troubleshoots/","text":"Problem 1: Libegl-mecha0 , unmet dependencies error 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 -desktop:~ sudo apt-get install libegl-mesa0 Reading package lists... Done Building dependency tree Reading state information... Done libegl-mesa0 is already the newest version ( 19 .2.8-0ubuntu0~18.04.3 ) . libegl-mesa0 set to manually installed. You might want to run 'apt --fix-broken install' to correct these. The following packages have unmet dependencies: libegl1:armhf : Depends: libegl-mesa0:armhf but it is not going to be installed E: Unmet dependencies. Try 'apt --fix-broken install' with no packages ( or specify a solution ) . -desktop:~ sudo apt-get install nano Reading package lists? Done Building dependency tree Reading state information? Done You might want to run ?apt --fix-broken install? to correct these. The following packages have unmet dependencies: libegl1:armhf : Depends: libegl-mesa0:armhf but it is not going to be installed E: Unmet dependencies. Try ?apt --fix-broken install? with no packages ( or specify a solution ) . -desktop:~$ sudo apt --fix-broken install Reading package lists? Done Building dependency tree Reading state information? Done Correcting dependencies? Done The following packages were automatically installed and are no longer required: iio-sensor-proxy:armhf libavahi-glib1:armhf libmbim-glib4 libmbim-glib4:armhf libmbim-proxy libmm-glib0:armhf libpolkit-gobject-1-0:armhf libqmi-glib5 libqmi-glib5:armhf libqmi-proxy modemmanager:armhf usb-modeswitch usb-modeswitch-data Use ?sudo apt autoremove? to remove them. The following additional packages will be installed: libegl-mesa0:armhf The following NEW packages will be installed: libegl-mesa0:armhf 0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded. 183 not fully installed or removed. Need to get 0 B/76.6 kB of archives. After this operation, 240 kB of additional disk space will be used. Do you want to continue ? [ Y/n ] Y debconf: delaying package configuration, since apt-utils is not installed ( Reading database ? 138736 files and directories currently installed. ) Preparing to unpack ?/libegl-mesa0_19.2.8-0ubuntu0~18.04.3_armhf.deb ? Unpacking libegl-mesa0:armhf ( 19 .2.8-0ubuntu0~18.04.3 ) ? dpkg: error processing archive /var/cache/apt/archives/libegl-mesa0_19.2.8-0ubuntu0~18.04.3_armhf.deb ( ?unpack ) : trying to overwrite shared ?/usr/share/glvnd/egl_vendor.d/50_mesa.json?, which is different from other instances of package libegl-mesa0:armhf Errors were encountered while processing: /var/cache/apt/archives/libegl-mesa0_19.2.8-0ubuntu0~18.04.3_armhf.deb E: Sub-process /usr/bin/dpkg returned an error code ( 1 ) solution : 1 2 3 4 -desktop:/usr/share/glvnd/egl_vendor.d$ sudo mv 50_mesa.json 50_mesa-old.json then ran fix install -desktop:/usr/share/glvnd/egl_vendor.d$ sudo apt ? fix-broken install ] https://medium.com/@hmurari/how-to-install-teamviewer-on-a-jetson-nano-38080f87f039 Problem 2: opencv_contrib installation -D OPENCV_GENERATE_PKGCONFIG=YES 1 sudo ldconfig -v sudo mv cv2.cpython-36m-x86_64-linux-gnu.so cv2.so","title":"Setup & Troubleshoot"},{"location":"Setup%20and%20Troubleshoots/#problem-1-libegl-mecha0-unmet-dependencies-error","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 -desktop:~ sudo apt-get install libegl-mesa0 Reading package lists... Done Building dependency tree Reading state information... Done libegl-mesa0 is already the newest version ( 19 .2.8-0ubuntu0~18.04.3 ) . libegl-mesa0 set to manually installed. You might want to run 'apt --fix-broken install' to correct these. The following packages have unmet dependencies: libegl1:armhf : Depends: libegl-mesa0:armhf but it is not going to be installed E: Unmet dependencies. Try 'apt --fix-broken install' with no packages ( or specify a solution ) . -desktop:~ sudo apt-get install nano Reading package lists? Done Building dependency tree Reading state information? Done You might want to run ?apt --fix-broken install? to correct these. The following packages have unmet dependencies: libegl1:armhf : Depends: libegl-mesa0:armhf but it is not going to be installed E: Unmet dependencies. Try ?apt --fix-broken install? with no packages ( or specify a solution ) . -desktop:~$ sudo apt --fix-broken install Reading package lists? Done Building dependency tree Reading state information? Done Correcting dependencies? Done The following packages were automatically installed and are no longer required: iio-sensor-proxy:armhf libavahi-glib1:armhf libmbim-glib4 libmbim-glib4:armhf libmbim-proxy libmm-glib0:armhf libpolkit-gobject-1-0:armhf libqmi-glib5 libqmi-glib5:armhf libqmi-proxy modemmanager:armhf usb-modeswitch usb-modeswitch-data Use ?sudo apt autoremove? to remove them. The following additional packages will be installed: libegl-mesa0:armhf The following NEW packages will be installed: libegl-mesa0:armhf 0 upgraded, 1 newly installed, 0 to remove and 13 not upgraded. 183 not fully installed or removed. Need to get 0 B/76.6 kB of archives. After this operation, 240 kB of additional disk space will be used. Do you want to continue ? [ Y/n ] Y debconf: delaying package configuration, since apt-utils is not installed ( Reading database ? 138736 files and directories currently installed. ) Preparing to unpack ?/libegl-mesa0_19.2.8-0ubuntu0~18.04.3_armhf.deb ? Unpacking libegl-mesa0:armhf ( 19 .2.8-0ubuntu0~18.04.3 ) ? dpkg: error processing archive /var/cache/apt/archives/libegl-mesa0_19.2.8-0ubuntu0~18.04.3_armhf.deb ( ?unpack ) : trying to overwrite shared ?/usr/share/glvnd/egl_vendor.d/50_mesa.json?, which is different from other instances of package libegl-mesa0:armhf Errors were encountered while processing: /var/cache/apt/archives/libegl-mesa0_19.2.8-0ubuntu0~18.04.3_armhf.deb E: Sub-process /usr/bin/dpkg returned an error code ( 1 ) solution : 1 2 3 4 -desktop:/usr/share/glvnd/egl_vendor.d$ sudo mv 50_mesa.json 50_mesa-old.json then ran fix install -desktop:/usr/share/glvnd/egl_vendor.d$ sudo apt ? fix-broken install ] https://medium.com/@hmurari/how-to-install-teamviewer-on-a-jetson-nano-38080f87f039","title":"Problem 1: Libegl-mecha0 , unmet dependencies error"},{"location":"Setup%20and%20Troubleshoots/#problem-2-opencv_contrib-installation","text":"-D OPENCV_GENERATE_PKGCONFIG=YES 1 sudo ldconfig -v sudo mv cv2.cpython-36m-x86_64-linux-gnu.so cv2.so","title":"Problem 2: opencv_contrib installation"},{"location":"Tips%20n%20Tricks/","text":"Cross-Compiling Enviroment Cuda toolkit L4T Useful link to follow: Install the IDE how to run camera for the first time Cross-Compiling Enviroment Cuda toolkit L4T run target via host-browser (USB and LAN) Jupyter activated Use Barrier to work with two monitor host and target, but with the same/one keyboard and mouse join some free course from Nvidia Deep Learning Institut, as follows: Fundamental ... Deepstream ... Useful link to follow: pysearchimage [jetsonhacks]: nvidia developer medium [cuda-tutorial.readthedocs.io]: https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/ \"readthedocs.io\" Install the IDE install spyder3 1 sudo apt-get install spyder3 install geany 1 sudo apt-get install geany opencv path configuration make new open cv, install sendiri, ikuti panduan nya dengan baik termasuk install opencv.contrib sekaligus https://docs.opencv.org/master/d7/d9f/tutorial_linux_install.html bisa membuat jetson nano hank, sebaiknya close semua app ?apakah openCV sudah terinstall di jetson nano? file Testing bisa how to run camera for the first time opencv.js aja https://docs.opencv.org/3.4/dd/d00/tutorial_js_video_display.html 1 2 3 v4l2-ctl is a command inside the package named v4l-utils. sudo apt-get install v4l-utils echo \"RAM,SWAP\" > stats.csv && tegrastats | while read a; do echo \"$a\" | awk -F \"[/ ]\" '{OFS=\",\"}{print $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21, $22, $23, $24, $25, $26, $27, $28, $29, $30, $31, $32}' >> stats.csv; done echo \"RAM,SWAP,CPU1,CPU2,CPU3,CPU4,GPU,TPLL,TCPU,TPMIC,TGPU,TAO,thermal,current\" > stats.csv && tegrastats | while read a; do echo \"$a\" | awk -F \"[/ ]\" '{OFS=\",\"}{print $2, $7, $12, $13, $14, $15, $19, $20, $21, $22, $23, $24, $25, $27}' >> stats.csv; done https://stackoverflow.com/questions/45713327/is-there-a-way-to-adjust-shutter-speed-or-exposure-time-of-a-webcam-using-python ^C https://gist.github.com/jwhendy/12bf558011fe5ff58bd5849954e84af4","title":"Tips & Tricks"},{"location":"Tips%20n%20Tricks/#cross-compiling-enviroment","text":"","title":"Cross-Compiling Enviroment"},{"location":"Tips%20n%20Tricks/#cuda-toolkit","text":"","title":"Cuda toolkit"},{"location":"Tips%20n%20Tricks/#l4t","text":"run target via host-browser (USB and LAN) Jupyter activated Use Barrier to work with two monitor host and target, but with the same/one keyboard and mouse join some free course from Nvidia Deep Learning Institut, as follows: Fundamental ... Deepstream ...","title":"L4T"},{"location":"Tips%20n%20Tricks/#useful-link-to-follow","text":"pysearchimage [jetsonhacks]: nvidia developer medium [cuda-tutorial.readthedocs.io]: https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/ \"readthedocs.io\"","title":"Useful link to follow:"},{"location":"Tips%20n%20Tricks/#install-the-ide","text":"install spyder3 1 sudo apt-get install spyder3 install geany 1 sudo apt-get install geany opencv path configuration make new open cv, install sendiri, ikuti panduan nya dengan baik termasuk install opencv.contrib sekaligus https://docs.opencv.org/master/d7/d9f/tutorial_linux_install.html bisa membuat jetson nano hank, sebaiknya close semua app ?apakah openCV sudah terinstall di jetson nano? file Testing bisa","title":"Install the IDE"},{"location":"Tips%20n%20Tricks/#how-to-run-camera-for-the-first-time","text":"opencv.js aja https://docs.opencv.org/3.4/dd/d00/tutorial_js_video_display.html 1 2 3 v4l2-ctl is a command inside the package named v4l-utils. sudo apt-get install v4l-utils echo \"RAM,SWAP\" > stats.csv && tegrastats | while read a; do echo \"$a\" | awk -F \"[/ ]\" '{OFS=\",\"}{print $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21, $22, $23, $24, $25, $26, $27, $28, $29, $30, $31, $32}' >> stats.csv; done echo \"RAM,SWAP,CPU1,CPU2,CPU3,CPU4,GPU,TPLL,TCPU,TPMIC,TGPU,TAO,thermal,current\" > stats.csv && tegrastats | while read a; do echo \"$a\" | awk -F \"[/ ]\" '{OFS=\",\"}{print $2, $7, $12, $13, $14, $15, $19, $20, $21, $22, $23, $24, $25, $27}' >> stats.csv; done https://stackoverflow.com/questions/45713327/is-there-a-way-to-adjust-shutter-speed-or-exposure-time-of-a-webcam-using-python ^C https://gist.github.com/jwhendy/12bf558011fe5ff58bd5849954e84af4","title":"how to run camera for the first time"},{"location":"_______________/","text":"","title":"___________________"},{"location":"about/","text":"","title":"About"},{"location":"aindeeplearning/","text":"Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout 1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to Febby Madrin's documentation"},{"location":"aindeeplearning/#welcome-to-febby-madrins-documentation","text":"For full documentation visit febbymadrin.github.io .","title":"Welcome to Febby Madrin's documentation"},{"location":"aindeeplearning/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"aindeeplearning/#commands_1","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout_1","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"aindeeplearning/#commands_2","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout_2","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"aindeeplearning/#commands_3","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout_3","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"aindeeplearning/#commands_4","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout_4","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"aindeeplearning/#commands_5","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout_5","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"aindeeplearning/#commands_6","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout_6","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"aindeeplearning/#commands_7","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout_7","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.# Welcome to Febby Madrin's documentation For full documentation visit febbymadrin.github.io .","title":"Project layout"},{"location":"aindeeplearning/#commands_8","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"aindeeplearning/#project-layout_8","text":"1 2 3 4 mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"binarization/","text":"Image Processing - Binarization Binarization var binarizationBaseURL = \"/javascripts/\"; Binarization is the process of converting a pixel image to a binary image: \"Neutrophils\" by Dr Graham Beards In the old days binarization was important for sending faxes. These days its still important for things like digitalising text or segmentation. At first the image is converted into grayscale: Load your own Image Then a threshold gets applied: Threshold 128 Adaptive Threshold The threshold can either be set fixed or adaptive using a clustering algorithm. This algorithm is called Iso Data Algorithm . It first counts the appearance of each tone in the image and tries to find a good center: Binarisation is the basis of segmentation .","title":"Binarization"},{"location":"binarization1/","text":"Binarization Binarization is the process of converting a pixel image to a binary image: \"Neutrophils\" by Dr Graham Beards In the old days binarization was important for sending faxes. These days its still important for things like digitalising text or segmentation. At first the image is converted into grayscale: Load your own Image Then a threshold gets applied: (Interactive Example) Threshold 128 Adaptive Threshold The threshold can either be set fixed or adaptive using a clustering algorithm. This algorithm is called Iso Data Algorithm . It first counts the appearance of each tone in the image and tries to find a good center: Binarisation is the basis of segmentation","title":"Binarization"},{"location":"binarization1/#binarization","text":"Binarization is the process of converting a pixel image to a binary image: \"Neutrophils\" by Dr Graham Beards In the old days binarization was important for sending faxes. These days its still important for things like digitalising text or segmentation. At first the image is converted into grayscale: Load your own Image","title":"Binarization"},{"location":"binarization1/#then-a-threshold-gets-applied-interactive-example","text":"Threshold 128 Adaptive Threshold The threshold can either be set fixed or adaptive using a clustering algorithm. This algorithm is called Iso Data Algorithm . It first counts the appearance of each tone in the image and tries to find a good center: Binarisation is the basis of segmentation","title":"Then a threshold gets applied: (Interactive Example)"},{"location":"coba/","text":"- Home: index.md - NVIDIA Courses: NVIDIA Courses.md - Niblack Binarization: Niblack Binarization.md - _ _ _ : _ ______.md - CUDA: CUDA Programming.md - OpenCV: OpenCV Programming.md - Setup & Troubleshoot: Setup and Troubleshoots.md - Tips & Tricks: Tips n Tricks.md - About: about.md - CV: CV.md site_name: Febby@Newtonbau nav: - Home: index.md - Niblack Binarization: Niblack Binarization.md - Deep Learning Tutorial: NVIDIA Courses.md - CUDA: CUDA Programming.md - OpenCV: OpenCV Programming.md - Setup & Troubleshoot: Setup and Troubleshoots.md - Tips & Tricks: Tips n Tricks.md - About: about.md - CV: CV.md - _ _ _ : _ ______.md theme: name: material markdown_extensions: - pymdownx.highlight: linenums: true linenums_style: pymdownx.inline - pymdownx.superfences - admonition - pymdownx.details extra: social: - icon: fontawesome/brands/linkedin link: https://www.linkedin.com/in/febbymadrin/","title":"- Home: index.md"},{"location":"coba/#-home-indexmd","text":"","title":"- Home: index.md"},{"location":"coba/#-nvidia-courses-nvidia-coursesmd","text":"","title":"- NVIDIA Courses: NVIDIA Courses.md"},{"location":"coba/#-niblack-binarization-niblack-binarizationmd","text":"","title":"- Niblack Binarization: Niblack Binarization.md"},{"location":"coba/#-___-_______md","text":"","title":"- ___: _______.md"},{"location":"coba/#-cuda-cuda-programmingmd","text":"","title":"- CUDA: CUDA Programming.md"},{"location":"coba/#-opencv-opencv-programmingmd","text":"","title":"- OpenCV: OpenCV Programming.md"},{"location":"coba/#-setup-troubleshoot-setup-and-troubleshootsmd","text":"","title":"- Setup &amp; Troubleshoot: Setup and Troubleshoots.md"},{"location":"coba/#-tips-tricks-tips-n-tricksmd","text":"","title":"- Tips &amp; Tricks: Tips n Tricks.md"},{"location":"coba/#-about-aboutmd","text":"","title":"- About: about.md"},{"location":"coba/#-cv-cvmd","text":"site_name: Febby@Newtonbau nav: - Home: index.md - Niblack Binarization: Niblack Binarization.md - Deep Learning Tutorial: NVIDIA Courses.md - CUDA: CUDA Programming.md - OpenCV: OpenCV Programming.md - Setup & Troubleshoot: Setup and Troubleshoots.md - Tips & Tricks: Tips n Tricks.md - About: about.md - CV: CV.md - _ _ _ : _ ______.md theme: name: material markdown_extensions: - pymdownx.highlight: linenums: true linenums_style: pymdownx.inline - pymdownx.superfences - admonition - pymdownx.details extra: social: - icon: fontawesome/brands/linkedin link: https://www.linkedin.com/in/febbymadrin/","title":"- CV: CV.md"},{"location":"imageregression/","text":"Classification Vs. Regression Unlike Image Classification applications, which map image inputs to discrete outputs (classes), the Image Regression task maps the image input pixels to continuous outputs. Continuous Outputs In the course regression project, those continuous outputs happen to define the X and Y coordinates of various features on a face, such as a nose. Mapping an image stream to a location for tracking can be used in other applications, such as following a line in mobile robotics. Tracking isn't the only thing a Regression model can do though. The output values could be something quite different such as steering values, or camera movement parameters. Changing The Final Layer The final layer of the pre-trained ResNet-18 network is a fully connected (fc) layer that has 512 inputs mapped to 1000 output classes, or (512, 1000). Using transfer learning in the Image Classification projects, that last layer was changed to only a few classes, depending on the application. For example, if there are to be 3 classes trained, we change the fc layer to (512, 3). The output includes the final layer of the neural network as a fully connected layer, with 512 inputs mapped to 3 classes. In the case of a Regression project predicting coordinates, we want two values for each category, the X and Y values. That means twice as many outputs are required in the fc layer. For example, if there are 3 facial features (nose, left_eye, right_eye), each with both an X and Y output, then 6 outputs are required, or (512, 6) for the fc layer. In classification, recall that the softmax function was used to build a probability distribution of the output values. For regression, we want to keep the actual values, because we didn't train for probabilities, but for actual X and Y output values. Evaluation Classification and Regression also differ in the way they are evaluated. The discrete values of classification can be evaluated based on accuracy, i.e. a calculation of the percentage of \"right\" answers. In the case of regression, we are interested in getting as close as possible to a correct answer. Therefore, the root mean squared error can be used. ---- Face XY Project The goal of this project is to build an Image Regression project that can predict the X and Y coordinates of a facial feature in a live image. Interactive Tool Startup Steps You will implement the project by collecting your own data using a clickable image display tool, training a model to find the XY coordinates of the feature, and then testing and updating your model as needed using images from the live camera. Since you are collecting two values for each category, the model may require more training and data to get a satisfactory result. Be patient! Building your model is an iterative process. Step 1: Open The Notebook To get started, navigate to the regression folder in your JupyterLab interface and double-click the regression_interactive.ipynb notebook to open it. Step 2: Execute All Of The Code Blocks The notebook is designed to be reusable for any XY regression task you wish to build. Step through the code blocks and execute them one at a time. Camera : This block sets the size of the images and starts the camera. If your camera is already active in this notebook or in another notebook, first shut down the kernel in the active notebook before running this code cell. Make sure that the correct camera type is selected for execution (USB). This cell may take several seconds to execute. Task : You get to define your TASK and CATEGORIES parameters here, as well as how many datasets you want to track. For the Face XY Project, this has already been defined for you as the face task with categories of nose, left_eye, and right_eye. Each category for the XY regression tool will require both an X and Y values. Go ahead and execute the cell. Subdirectories for each category are created to store the example images you collect. The file names of the images will contain the XY coordinates that you tag the images with during the data collection step. This cell should only take a few seconds to execute. Data Collection : You?ll collect images for your categories with a special clickable image widget set up in this cell. As you click the ?nose? or ?eye? in the live feed image, the data image filename is automatically annotated and saved using the X and Y coordinates from the click. Model : The model is set to the same pre-trained ResNet18 model for this project: 1 model = torchvision.models.resnet18(pretrained=True) For more information on available PyTorch pre-trained models, see the PyTorch documentation. In addition to choosing the model, the last layer of the model is modified to accept only the number of classes that we are training for. In the case of the Face XY Project, it is twice the number of categories, since each requires both X and Y coordinates (i.e. nose X, nose Y, left_eye X, right_eye X and right_eye Y). 1 2 output_dim = 2 * len(dataset.categories) model.fc = torch.nn.Linear(512, output_dim) This code cell may take several seconds to execute. Live Execution : This code block sets up threading to run the model in the background so that you can view the live camera feed and visualize the model performance in real time. This cell should only take a few seconds to execute. For this project,a blue circle will overlay the model prediction for the location of the feature selected. Training and Evaluation : The training code cell sets the hyper-parameters for the model training (number of epochs, batch size, learning rate, momentum) and loads the images for training or evaluation. The regression version is very similar to the simple classification training, though the loss is calculated differently. The mean square error over the X and Y value errors is calculated and used as the loss for backpropagation in training to improve the model. This code cell may take several seconds to execute. Display the Interactive Tool! : This is the last code cell. All that's left to do is pack all the widgets into one comprehensive tool and display it. This cell may take several seconds to run and should display the full tool for you to work with. There are three image windows. Initially, only the left camera feed is populated. The middle window will display the most recent annotated snapshot image once you start collecting data. The right-most window will display the live prediction view once the model has been trained. face_xy_interactive_tool Step 3: Collect Data, Train, Test Position the camera in front of your face and collect initial data. Point to the target feature with the mouse cursor that matches the category you've selected (such as the nose). Click to collect data. The annotated snapshot you just collected will appear in the middle display box. As you collect each image, vary your head position and pose: Add 20 images of your nose with the nose category selected Add 20 images of your left eye face with the left_eye category selected Add 20 images of your right eye with the right_eye category selected Set the number of epochs to 10 and click the train button Once the training is complete, try the live view and observe the prediction. A blue circle should appear on the feature selected. Step 4: Improve Your Model Use the live inference as a guide to improve your model! The live feed shows the model's prediction. As you move your head, does the target circle correctly follow your nose (or left_eye, right_eye)? If not, then click the correct location and add data. After you've added some data for a new scenario, train the model some more. For example: Move the camera so that the face is closer. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training. Move the camera to provide a different background. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training. Are there any other scenarios you think the model might not perform well? Try them out! Can you get a friend to try your model? Does it work the same? You know the drill: more data and training! Step 5: Save Your Model When you are satisfied with your model, save it by entering a name in the \"model path\" box and click \"save model\". More Regression Projects To build another project, follow the pattern you did with the Face Project. Save your previous work, modify the TASK and CATEGORIES values, shutdown and restart the notebook, and run all the cells. Then collect, train, and test!","title":"Image Regression"},{"location":"imageregression/#classification-vs-regression","text":"Unlike Image Classification applications, which map image inputs to discrete outputs (classes), the Image Regression task maps the image input pixels to continuous outputs. Continuous Outputs In the course regression project, those continuous outputs happen to define the X and Y coordinates of various features on a face, such as a nose. Mapping an image stream to a location for tracking can be used in other applications, such as following a line in mobile robotics. Tracking isn't the only thing a Regression model can do though. The output values could be something quite different such as steering values, or camera movement parameters. Changing The Final Layer The final layer of the pre-trained ResNet-18 network is a fully connected (fc) layer that has 512 inputs mapped to 1000 output classes, or (512, 1000). Using transfer learning in the Image Classification projects, that last layer was changed to only a few classes, depending on the application. For example, if there are to be 3 classes trained, we change the fc layer to (512, 3). The output includes the final layer of the neural network as a fully connected layer, with 512 inputs mapped to 3 classes. In the case of a Regression project predicting coordinates, we want two values for each category, the X and Y values. That means twice as many outputs are required in the fc layer. For example, if there are 3 facial features (nose, left_eye, right_eye), each with both an X and Y output, then 6 outputs are required, or (512, 6) for the fc layer. In classification, recall that the softmax function was used to build a probability distribution of the output values. For regression, we want to keep the actual values, because we didn't train for probabilities, but for actual X and Y output values. Evaluation Classification and Regression also differ in the way they are evaluated. The discrete values of classification can be evaluated based on accuracy, i.e. a calculation of the percentage of \"right\" answers. In the case of regression, we are interested in getting as close as possible to a correct answer. Therefore, the root mean squared error can be used.","title":"Classification Vs. Regression"},{"location":"imageregression/#-","text":"","title":"----"},{"location":"imageregression/#face-xy-project","text":"The goal of this project is to build an Image Regression project that can predict the X and Y coordinates of a facial feature in a live image.","title":"Face XY Project"},{"location":"imageregression/#interactive-tool-startup-steps","text":"You will implement the project by collecting your own data using a clickable image display tool, training a model to find the XY coordinates of the feature, and then testing and updating your model as needed using images from the live camera. Since you are collecting two values for each category, the model may require more training and data to get a satisfactory result. Be patient! Building your model is an iterative process. Step 1: Open The Notebook To get started, navigate to the regression folder in your JupyterLab interface and double-click the regression_interactive.ipynb notebook to open it. Step 2: Execute All Of The Code Blocks The notebook is designed to be reusable for any XY regression task you wish to build. Step through the code blocks and execute them one at a time. Camera : This block sets the size of the images and starts the camera. If your camera is already active in this notebook or in another notebook, first shut down the kernel in the active notebook before running this code cell. Make sure that the correct camera type is selected for execution (USB). This cell may take several seconds to execute. Task : You get to define your TASK and CATEGORIES parameters here, as well as how many datasets you want to track. For the Face XY Project, this has already been defined for you as the face task with categories of nose, left_eye, and right_eye. Each category for the XY regression tool will require both an X and Y values. Go ahead and execute the cell. Subdirectories for each category are created to store the example images you collect. The file names of the images will contain the XY coordinates that you tag the images with during the data collection step. This cell should only take a few seconds to execute. Data Collection : You?ll collect images for your categories with a special clickable image widget set up in this cell. As you click the ?nose? or ?eye? in the live feed image, the data image filename is automatically annotated and saved using the X and Y coordinates from the click. Model : The model is set to the same pre-trained ResNet18 model for this project: 1 model = torchvision.models.resnet18(pretrained=True) For more information on available PyTorch pre-trained models, see the PyTorch documentation. In addition to choosing the model, the last layer of the model is modified to accept only the number of classes that we are training for. In the case of the Face XY Project, it is twice the number of categories, since each requires both X and Y coordinates (i.e. nose X, nose Y, left_eye X, right_eye X and right_eye Y). 1 2 output_dim = 2 * len(dataset.categories) model.fc = torch.nn.Linear(512, output_dim) This code cell may take several seconds to execute. Live Execution : This code block sets up threading to run the model in the background so that you can view the live camera feed and visualize the model performance in real time. This cell should only take a few seconds to execute. For this project,a blue circle will overlay the model prediction for the location of the feature selected. Training and Evaluation : The training code cell sets the hyper-parameters for the model training (number of epochs, batch size, learning rate, momentum) and loads the images for training or evaluation. The regression version is very similar to the simple classification training, though the loss is calculated differently. The mean square error over the X and Y value errors is calculated and used as the loss for backpropagation in training to improve the model. This code cell may take several seconds to execute. Display the Interactive Tool! : This is the last code cell. All that's left to do is pack all the widgets into one comprehensive tool and display it. This cell may take several seconds to run and should display the full tool for you to work with. There are three image windows. Initially, only the left camera feed is populated. The middle window will display the most recent annotated snapshot image once you start collecting data. The right-most window will display the live prediction view once the model has been trained. face_xy_interactive_tool Step 3: Collect Data, Train, Test Position the camera in front of your face and collect initial data. Point to the target feature with the mouse cursor that matches the category you've selected (such as the nose). Click to collect data. The annotated snapshot you just collected will appear in the middle display box. As you collect each image, vary your head position and pose: Add 20 images of your nose with the nose category selected Add 20 images of your left eye face with the left_eye category selected Add 20 images of your right eye with the right_eye category selected Set the number of epochs to 10 and click the train button Once the training is complete, try the live view and observe the prediction. A blue circle should appear on the feature selected. Step 4: Improve Your Model Use the live inference as a guide to improve your model! The live feed shows the model's prediction. As you move your head, does the target circle correctly follow your nose (or left_eye, right_eye)? If not, then click the correct location and add data. After you've added some data for a new scenario, train the model some more. For example: Move the camera so that the face is closer. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training. Move the camera to provide a different background. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training. Are there any other scenarios you think the model might not perform well? Try them out! Can you get a friend to try your model? Does it work the same? You know the drill: more data and training! Step 5: Save Your Model When you are satisfied with your model, save it by entering a name in the \"model path\" box and click \"save model\".","title":"Interactive Tool Startup Steps"},{"location":"imageregression/#more-regression-projects","text":"To build another project, follow the pattern you did with the Face Project. Save your previous work, modify the TASK and CATEGORIES values, shutdown and restart the notebook, and run all the cells. Then collect, train, and test!","title":"More Regression Projects"},{"location":"rencana/","text":"HOME Download CV Documentation Microcontroller ESP8266 ESP32 STM32 C++ Qt Unreal Engine Python Django Flask Javascript ReactJS SPARK AR AI and Deeplearning NVIDIA Jetson Nano Blender","title":"Rencana"},{"location":"setup/","text":"Introduction And Download The NVIDIA\u00ae Jetson Nano\u2122 Developer Kit is a small AI computer for makers, learners, and developers. After following along with this brief guide, you\u2019ll be ready to start building practical AI applications, cool AI robots, and more. Download Initially, a computer with Internet connection and the ability to flash your microSD card is also required. Downloading the image may take a considerable amount of time, depending on your Internet speed. We recommend that you begin downloading the NVIDIA DLI AI Jetson Nano SD Card Image v1.1.1 immediately so that it can work in the background while you explore the rest of the setup or course information. Included In The Box Your Jetson Nano Developer Kit box includes: Jetson Nano Developer Kit Small paper card with quick start and support information Folded paper stand For This Course, You\u2019ll Also Need: microSD Memory Card (32GB UHS-I minimum) 5V 4A Power Supply with 2.1mm DC barrel connector 2-pin Jumper Compatible camera: Logitech C270 USB Webcam USB cable (Micro-B to Type-A) ---- Prepare For Setup Items For Getting Started MicroSD Card The Jetson Nano Developer Kit uses a microSD card as a boot device and for main storage. It\u2019s important to have a card that\u2019s fast and large enough for your projects; the minimum recommended for this course is a 32GB UHS-I card. See the instructions in the \"Write Image to the microSD Card\" lesson that follows to flash your microSD card according to the type of computer you are using: Windows, Mac, or Linux. 5V 4A Power Supply With 2.1mm DC Barrel Connector For this course, the 5V 4A DC barrel jack power supply is required. Although it is possible to power the Jetson Nano with a smaller microUSB supply, this is not robust enough for the high GPU compute load we require for our projects. In addition, you will need the microUSB port available as a direct connection to your computer for this course. The barrel jack must be 5.5mm OD x 2.1mm ID x 9.5mm length, center-positive. As an example of a good power supply, NVIDIA has validated Adafruit\u2019s 5V 4A (4000mA) switching power supply - UL Listed. 2-Pin Jumper To specify use of the barrel-type power supply on the Jetson Nano Developer Kit, a 2-pin jumper is required. This is an inexpensive item available at many outlets. Logitech C270 USB Webcam You'll need a camera to capture images in the course projects. As an example of a compatible camera, NVIDIA has verified that the Logitech C270 USB Webcam works with these projects. The ability to position the camera easily for capturing images hands-free makes this a great choice. Some other USB webcams may also work with the projects. If you already have one on hand, you could test it as an alternative. USB Cable (Micro-B To Type-A) You'll also need a Micro USB to USB-A cable to directly connect your computer to the Jetson Nano Developer Kit's Micro USB port. The cable must be capable of data transfers, rather than only designed to power a device. This is a common cable available at many outlets if you don't already have one on hand. The complete hardware package is also available from Sparkfun either with the Jetson Nano included or without the Jetson Nano included. ---- Write Image To The MicroSD Card To prepare your microSD card, you\u2019ll need a computer with Internet connection and the ability to read and write SD cards, either via a built-in SD card slot or adapter. If you have not already done so, download the DLI AI Jetson Nano SD Card Image to flash from the Introduction and Download page, and note where it was saved on the computer. Write the image to your microSD card by following the instructions below according to the type of computer you are using: Windows, Mac, or Linux. INSTRUCTIONS FOR WINDOWS After your microSD card is ready, proceed to set up your developer kit. Format your microSD card using SD Memory Card Formatter from the SD Association. Download, install, and launch SD Memory Card Formatter for Windows. Select card drive Select \u201cQuick format\u201d Leave \u201cVolume label\u201d blank Click \u201cFormat\u201d to start formatting, and \u201cYes\u201d on the warning dialog Use Etcher to write the Jetson Nano Developer Kit SD Card Image to your microSD card Download, install, and launch Etcher. Click \u201cSelect image\u201d and choose the zipped image file downloaded earlier. Insert your microSD card if not already inserted. Click Cancel (per this explanation) if Windows prompts you with a dialog like this: Click \u201cSelect drive\u201d and choose the correct device. Click \u201cFlash!\u201d It will take Etcher about 10 minutes to write and validate the image if your microSD card is connected via USB3. After Etcher finishes, Windows may let you know it doesn\u2019t know how to read the SD Card. Just click Cancel and remove the microSD card. INSTRUCTIONS FOR MAC You can either write the SD card image using a graphical program like Etcher, or via command line. Etcher Instructions Do not insert your microSD card yet. Download, install, and launch Etcher. Click \u201cSelect image\u201d and choose the zipped image file downloaded earlier. Insert your microSD card. Click Ignore if your Mac shows this window: If you have no other external drives attached, Etcher will automatically select the microSD card as target device. Otherwise, click \u201cSelect drive\u201d and choose the correct device. Click \u201cFlash!\u201d Your Mac may prompt for your username and password before it allows Etcher to proceed It will take Etcher about 10 minutes to write and validate the image if your microSD card is connected via USB3. After Etcher finishes, your Mac may let you know it doesn\u2019t know how to read the SD Card. Just click Eject and remove the microSD card. Command Line Instructions Do not insert your microSD card yet. Waiting will help you discover correct disk device name in steps below. Open the Terminal app: Use this command to list any external disk devices already attached to your Mac: diskutil list external | fgrep '/dev/disk' For example, if you already have a USB drive attached to your Mac, the result will look similar to this: Insert your microSD card. Click Ignore if your Mac shows this window: Use the same command as before to list external disk devices. The newly listed disk device is the microSD card (/dev/disk2 in this example): Use this command to remove any existing partitions from the microSD card, ensuring MacOS will let you write to it. BE VERY CAREFUL to specify the correct disk device. bash sudo diskutil partitionDisk /dev/disk<n> 1 GPT \"Free Space\" \"%noformat%\" 100% For example: Use this command to write the zipped SD card image to the microSD card. Note the use of /dev/rdisk instead of /dev/disk: bash /usr/bin/unzip -p ~/Downloads/jetson_nano_devkit_sd_card.zip | sudo /bin/dd of=/dev/rdisk<n>bs=1m For example: There will be no indication of progress (unless you signal with CTRL-t). When the dd command finishes, your Mac will let you know it cannot read the microSD card. Just click Eject: INSTRUCTIONS FOR LINUX You can either write the SD card image using a graphical program like Etcher, or via command line. Etcher Instructions Download, install, and launch Etcher. Click \u201cSelect image\u201d and choose the zipped image file downloaded earlier. Insert your microSD card. If you have no other external drives attached, Etcher will automatically select the microSD card as target device. Otherwise, click \u201cChange\u201d and choose the correct device. Click \u201cFlash!\u201d Your OS may prompt for your username and password before it allows Etcher to proceed. It will take Etcher 10-15 minutes to write and validate the image if your microSD card is connected via USB3. After Etcher finishes, eject the SD Card using Files application: Physically remove microSD card from the computer. Command Line Instructions Open the Terminal application by pressing Ctrl + Alt + Del Insert your microSD card, then use a command like this to show which disk device was assigned to it: bash dmesg | tail | awk '$3 == \"sd\" {print}' In this example, we can see the 16GB microSD card was assigned /dev/sda: Use this command to write the zipped SD card image to the microSD card: bash /usr/bin/unzip -p ~/Downloads/jetson_nano_devkit_sd_card.zip | sudo /bin/dd of=/dev/sd<x> bs=1M status=progress For example: When the dd command finishes, eject the disk device from the command line: bash sudo eject /dev/sd<x> Physically remove microSD card from the computer. ---- Setup And First Boot Headless Device Mode For this course, we are running the Jetson Nano Developer Kit in a \"headless\" configuration. That means you do not hook up a monitor directly to the Jetson Nano Developer Kit. This method conserves memory resources on the Jetson Nano and has the added benefit of eliminating the requirement for extra hardware, i.e. a monitor, keyboard, and mouse. In addition, we will further simplify the configuration by using \"USB Device Mode\". In this mode, your Jetson Nano Developer Kit connects directly to your computer through a USB cable. This eliminates the need for a network connection on the Jetson Nano, as well as the need to determine the IP address on your network. It is always 192.168.55.1:8888 in this mode. In the steps that follow, you will boot the Jetson Nano in the minimum configuration (without a camera) to make sure it boots correctly from the microSD card you flashed with the DLI course image. Setup Steps Unfold the paper stand and place inside the developer kit box. Set the developer kit on top of the paper stand. Insert the microSD card (with system image already written to it) into the slot on the underside of the Jetson Nano module. Insert the 2-pin jumper across the 2-pin connector, J48, located next to the MIPI CSI camera connector. This enables the DC barrel power supply. Connect your DC barrel jack power supply (5V/4A). The Jetson Nano Developer Kit will power on and boot automatically. A green LED next to the Micro-USB connector will light as soon as the developer kit powers on. Wait about 30 seconds. Then connect the USB cable from the Micro USB port on the Jetson Nano Developer Kit to the USB port on your computer. Logging Into The JupyterLab Server Open the following link address : 192.168.55.1:8888 The JupyterLab server running on the Jetson Nano will open up with a login prompt the first time. Enter the password: dlinano You will see this screen. Congratulations! Headless Device Mode Setup For Jetson Nano Demonstration Troubleshooting The LED does not light up when the DC barrel jack power supply is connected. Check to be sure you have shorted the two pins on the J48 header with a jumper. The LED lights up, but I cannot access the JupyterLab server from my browser. Try a different browser. I cannot access the JupyterLab server from any browser. Check your computer to see if any new USB devices are recognized when plugging in the USB cable to your computer If on Windows, check ?Device Manager? to see if any new device was added. My computer does not recognize Jetson Nano when connected. Check your USB cable to see if it is enabled for data transfer. You can test it by connecting the Micro-B end of the USB cable to some other USB peripheral such as tablet, Kindle, or other device that communicates over a USB Micro-B port. My USB cable seems good, but my computer does not recognize Jetson Nano. Check if your Jetson Nano Developer Kit is properly booting by connecting it to a TV through an HDMI cable. See if the TV displays the NVIDIA logo when booted, and eventually displays the Ubuntu desktop. My Jetson Nano does not show anything on the TV when booting with the TV attached. Check if you have inserted your microSD card all the way into the microSD card slot. You should hear a small click sound. The microSD card is fully inserted, but my Jetson Nano does not boot properly. Go back to ?Write Image To The MicroSD Card? to reflash your SD card. ---- Camera Setup Now that you've verified that your system can boot to your Jetson Nano Developer Kit with the microSD card, let's add the camera! Power down the Jetson Nano Developer Kit by unplugging the power. Then connect the camera using the instructions below. Connecting The Logitech C270 Webcam (Recommended Configuration) This is very straightforward. Just plug in the USB connector into any of the Jetson Nano Developer Kit USB ports. (Alternate Configuration) Raspberry Pi V2 Camera If you want to try this lab with a RasBerry Pi v2 Camera, you will need to connect to the MIPI CSI port. Begin by unlatching the MIPI CSI connector. This loosens the \"grip\" of the connector by just a small amount. Insert the ribbon cable of the camera so that the metal side faces into the Nano board. Latch the connector with a gentle push downward on the sides of the plastic. The ribbon cable should be securely held by the connector Remove the protective film from the lens of the camera. Other Cameras Other cameras may also work with your Jetson Nano Developer Kit. You'll need to test them to find out. If you have another camera on hand, such as a USB webcam, feel free to give it a try using the \"Hello Camera\" test notebooks in the next lesson. Raspberry Pi Camera Connection Demonstration Troubleshooting The camera and JupyterLab appear \"frozen\" If using a Raspberry Pi Camera Module v2, check to ensure it does not touch any of the metal parts (headers, pads, t erminals, ports) of the Jetson Nano Developer Kit board as this may cause an electrical short. JupyterLab is working, but I cannot execute a cell to run my camera. I get an error or it \"hangs\" The camera may have previously been assigned but not released. Shutdown the kernel for the notebook using the pulldown menu at the top of JupyterLab. Open a terminal window from the Launch page (if no Launch page, click the '+' icon) enter sudo systemctl restart nvargus-daemon in the terminal window. You will be prompted for the password, which is dlinano Restart your notebook ---- Hello Camera Now with the camera attached, boot the system using headless Device Mode as you did before. Boot With Camera Attached Disconnect the USB cable from the computer if it is still attached Connect your DC barrel jack power supply (5V/4A). The Jetson Nano Developer Kit will power on and boot automatically. Wait about 30 seconds. Then connect the USB cable from the Micro USB port on the Jetson Nano Developer Kit to the USB port on your computer. Open a browser window on your computer and enter the address to the Jetson Nano JupyterLab server: 192.168.55.1:8888 The JupyterLab server running on the Jetson Nano will open up. Open The Hello Camera Notebook The JupyterLab interface is a dashboard that provides access to the Jupyter interactive notebooks and the operating system for Jetson Nano. The first view you'll see includes a directory tree on the left and a \"Launcher\" page on the right. To open the \"Hello Camera\" notebook: Navigate to the nvdli-nano folder with a double-click Navigate to the hello_camera folder in the same way If you are testing a USB webcam camera such as the Logitech C270, double-click the usb_camera.ipynb notebook to open it. If you are testing a CSI camera, double-click the csi_camera.ipynb instead. Find out more about JupyterLab in the next section. If you are already familiar with JupyterLab features, go ahead and jump right in! When you're satisfied that your camera works correctly, return here for project instructions. ---- JupyterLab For this course, your Jetson Nano has been configured to run a JupyterLab server on port 8888. When you boot the system and open a browser to the Jetson Nano IP address at that port, you see the JupyterLab interface. JupyterLab Interface The JupyterLab Interface is a dashboard that provides access to interactive iPython notebooks, as well as the folder structure for your Jetson Nano and a terminal window into the Ubuntu operating system. The first view you'll see includes a menu bar at the top, a directory tree in the left sidebar, and a main work area that is initially open to the \"Launcher\" page. Complete details for all the features and menu actions available can be found in the JupyterLab Interface document. Here are some key capabilities that will be especially useful in this course: File browser: The file browser in the left sidebar allows navigation through the Jetson Nano file structure. Double-clicking on a notebook or file opens it in the main work area. iPython notebooks: The interactive notebooks used in this course have an \".ipynb\" file extension. When a notebook is double-clicked from the file browser, it will open in the main work area and its process will start. The notebooks consist of text and code \"cells\". When a code cell is \"run\", by clicking the run button at the top of the notebook or the keyboard shortcut [CTRL][ENTER], the block of code in the cell is executed and the output, if there is any, appears below the cell in the notebook. To the left of each executable cell there is an \"execution count\" or \"prompt number\" in brackets. If the cell takes more than a few seconds to run, you will see an asterisk mark there, indicating that the cell has not finished its execution. Once processing of that cell is finished, a number will show in the brackets. Kernel operations: The kernel for each running notebook is a separate process that runs the user code. The kernel starts automatically when the notebook is opened from the file browser. The kernel menu on the main menu bar includes commands to shutdown or restart the kernel, which you will need to use periodically. After a kernel shutdown, no code cells can be executed. When a kernel is restarted, all memory is lost regarding imported packages, variable assignments, and so on. Cell tabs: You can move any cell to new window tabs in the main work area by right-clicking the cell and selecting \"Create New View for Output\". This way, you can continue to scroll down the JupyterLab notebook while still watching a particular cell. This is especially helpful in the cell includes a camera view! Terminal window: You can work directly in a Terminal window on your Jetson Nano Ubuntu OS. From the Launcher page, click the Terminal icon under \"Other\". To bring up the Launcher page, if it is no longer visible, click the \"+\" icon at the top of the left sidebar.","title":"Setting up the Jetson Nano"},{"location":"setup/#introduction-and-download","text":"The NVIDIA\u00ae Jetson Nano\u2122 Developer Kit is a small AI computer for makers, learners, and developers. After following along with this brief guide, you\u2019ll be ready to start building practical AI applications, cool AI robots, and more. Download Initially, a computer with Internet connection and the ability to flash your microSD card is also required. Downloading the image may take a considerable amount of time, depending on your Internet speed. We recommend that you begin downloading the NVIDIA DLI AI Jetson Nano SD Card Image v1.1.1 immediately so that it can work in the background while you explore the rest of the setup or course information.","title":"Introduction And Download"},{"location":"setup/#included-in-the-box","text":"Your Jetson Nano Developer Kit box includes: Jetson Nano Developer Kit Small paper card with quick start and support information Folded paper stand For This Course, You\u2019ll Also Need: microSD Memory Card (32GB UHS-I minimum) 5V 4A Power Supply with 2.1mm DC barrel connector 2-pin Jumper Compatible camera: Logitech C270 USB Webcam USB cable (Micro-B to Type-A)","title":"Included In The Box"},{"location":"setup/#-","text":"","title":"----"},{"location":"setup/#prepare-for-setup","text":"","title":"Prepare For Setup"},{"location":"setup/#items-for-getting-started","text":"MicroSD Card The Jetson Nano Developer Kit uses a microSD card as a boot device and for main storage. It\u2019s important to have a card that\u2019s fast and large enough for your projects; the minimum recommended for this course is a 32GB UHS-I card. See the instructions in the \"Write Image to the microSD Card\" lesson that follows to flash your microSD card according to the type of computer you are using: Windows, Mac, or Linux. 5V 4A Power Supply With 2.1mm DC Barrel Connector For this course, the 5V 4A DC barrel jack power supply is required. Although it is possible to power the Jetson Nano with a smaller microUSB supply, this is not robust enough for the high GPU compute load we require for our projects. In addition, you will need the microUSB port available as a direct connection to your computer for this course. The barrel jack must be 5.5mm OD x 2.1mm ID x 9.5mm length, center-positive. As an example of a good power supply, NVIDIA has validated Adafruit\u2019s 5V 4A (4000mA) switching power supply - UL Listed. 2-Pin Jumper To specify use of the barrel-type power supply on the Jetson Nano Developer Kit, a 2-pin jumper is required. This is an inexpensive item available at many outlets. Logitech C270 USB Webcam You'll need a camera to capture images in the course projects. As an example of a compatible camera, NVIDIA has verified that the Logitech C270 USB Webcam works with these projects. The ability to position the camera easily for capturing images hands-free makes this a great choice. Some other USB webcams may also work with the projects. If you already have one on hand, you could test it as an alternative. USB Cable (Micro-B To Type-A) You'll also need a Micro USB to USB-A cable to directly connect your computer to the Jetson Nano Developer Kit's Micro USB port. The cable must be capable of data transfers, rather than only designed to power a device. This is a common cable available at many outlets if you don't already have one on hand. The complete hardware package is also available from Sparkfun either with the Jetson Nano included or without the Jetson Nano included.","title":"Items For Getting Started"},{"location":"setup/#-_1","text":"","title":"----"},{"location":"setup/#write-image-to-the-microsd-card","text":"To prepare your microSD card, you\u2019ll need a computer with Internet connection and the ability to read and write SD cards, either via a built-in SD card slot or adapter. If you have not already done so, download the DLI AI Jetson Nano SD Card Image to flash from the Introduction and Download page, and note where it was saved on the computer. Write the image to your microSD card by following the instructions below according to the type of computer you are using: Windows, Mac, or Linux. INSTRUCTIONS FOR WINDOWS After your microSD card is ready, proceed to set up your developer kit. Format your microSD card using SD Memory Card Formatter from the SD Association. Download, install, and launch SD Memory Card Formatter for Windows. Select card drive Select \u201cQuick format\u201d Leave \u201cVolume label\u201d blank Click \u201cFormat\u201d to start formatting, and \u201cYes\u201d on the warning dialog Use Etcher to write the Jetson Nano Developer Kit SD Card Image to your microSD card Download, install, and launch Etcher. Click \u201cSelect image\u201d and choose the zipped image file downloaded earlier. Insert your microSD card if not already inserted. Click Cancel (per this explanation) if Windows prompts you with a dialog like this: Click \u201cSelect drive\u201d and choose the correct device. Click \u201cFlash!\u201d It will take Etcher about 10 minutes to write and validate the image if your microSD card is connected via USB3. After Etcher finishes, Windows may let you know it doesn\u2019t know how to read the SD Card. Just click Cancel and remove the microSD card. INSTRUCTIONS FOR MAC You can either write the SD card image using a graphical program like Etcher, or via command line. Etcher Instructions Do not insert your microSD card yet. Download, install, and launch Etcher. Click \u201cSelect image\u201d and choose the zipped image file downloaded earlier. Insert your microSD card. Click Ignore if your Mac shows this window: If you have no other external drives attached, Etcher will automatically select the microSD card as target device. Otherwise, click \u201cSelect drive\u201d and choose the correct device. Click \u201cFlash!\u201d Your Mac may prompt for your username and password before it allows Etcher to proceed It will take Etcher about 10 minutes to write and validate the image if your microSD card is connected via USB3. After Etcher finishes, your Mac may let you know it doesn\u2019t know how to read the SD Card. Just click Eject and remove the microSD card. Command Line Instructions Do not insert your microSD card yet. Waiting will help you discover correct disk device name in steps below. Open the Terminal app: Use this command to list any external disk devices already attached to your Mac: diskutil list external | fgrep '/dev/disk' For example, if you already have a USB drive attached to your Mac, the result will look similar to this: Insert your microSD card. Click Ignore if your Mac shows this window: Use the same command as before to list external disk devices. The newly listed disk device is the microSD card (/dev/disk2 in this example): Use this command to remove any existing partitions from the microSD card, ensuring MacOS will let you write to it. BE VERY CAREFUL to specify the correct disk device. bash sudo diskutil partitionDisk /dev/disk<n> 1 GPT \"Free Space\" \"%noformat%\" 100% For example: Use this command to write the zipped SD card image to the microSD card. Note the use of /dev/rdisk instead of /dev/disk: bash /usr/bin/unzip -p ~/Downloads/jetson_nano_devkit_sd_card.zip | sudo /bin/dd of=/dev/rdisk<n>bs=1m For example: There will be no indication of progress (unless you signal with CTRL-t). When the dd command finishes, your Mac will let you know it cannot read the microSD card. Just click Eject: INSTRUCTIONS FOR LINUX You can either write the SD card image using a graphical program like Etcher, or via command line. Etcher Instructions Download, install, and launch Etcher. Click \u201cSelect image\u201d and choose the zipped image file downloaded earlier. Insert your microSD card. If you have no other external drives attached, Etcher will automatically select the microSD card as target device. Otherwise, click \u201cChange\u201d and choose the correct device. Click \u201cFlash!\u201d Your OS may prompt for your username and password before it allows Etcher to proceed. It will take Etcher 10-15 minutes to write and validate the image if your microSD card is connected via USB3. After Etcher finishes, eject the SD Card using Files application: Physically remove microSD card from the computer. Command Line Instructions Open the Terminal application by pressing Ctrl + Alt + Del Insert your microSD card, then use a command like this to show which disk device was assigned to it: bash dmesg | tail | awk '$3 == \"sd\" {print}' In this example, we can see the 16GB microSD card was assigned /dev/sda: Use this command to write the zipped SD card image to the microSD card: bash /usr/bin/unzip -p ~/Downloads/jetson_nano_devkit_sd_card.zip | sudo /bin/dd of=/dev/sd<x> bs=1M status=progress For example: When the dd command finishes, eject the disk device from the command line: bash sudo eject /dev/sd<x> Physically remove microSD card from the computer.","title":"Write Image To The MicroSD Card"},{"location":"setup/#-_2","text":"","title":"----"},{"location":"setup/#setup-and-first-boot","text":"","title":"Setup And First Boot"},{"location":"setup/#headless-device-mode","text":"For this course, we are running the Jetson Nano Developer Kit in a \"headless\" configuration. That means you do not hook up a monitor directly to the Jetson Nano Developer Kit. This method conserves memory resources on the Jetson Nano and has the added benefit of eliminating the requirement for extra hardware, i.e. a monitor, keyboard, and mouse. In addition, we will further simplify the configuration by using \"USB Device Mode\". In this mode, your Jetson Nano Developer Kit connects directly to your computer through a USB cable. This eliminates the need for a network connection on the Jetson Nano, as well as the need to determine the IP address on your network. It is always 192.168.55.1:8888 in this mode. In the steps that follow, you will boot the Jetson Nano in the minimum configuration (without a camera) to make sure it boots correctly from the microSD card you flashed with the DLI course image.","title":"Headless Device Mode"},{"location":"setup/#setup-steps","text":"Unfold the paper stand and place inside the developer kit box. Set the developer kit on top of the paper stand. Insert the microSD card (with system image already written to it) into the slot on the underside of the Jetson Nano module. Insert the 2-pin jumper across the 2-pin connector, J48, located next to the MIPI CSI camera connector. This enables the DC barrel power supply. Connect your DC barrel jack power supply (5V/4A). The Jetson Nano Developer Kit will power on and boot automatically. A green LED next to the Micro-USB connector will light as soon as the developer kit powers on. Wait about 30 seconds. Then connect the USB cable from the Micro USB port on the Jetson Nano Developer Kit to the USB port on your computer.","title":"Setup Steps"},{"location":"setup/#logging-into-the-jupyterlab-server","text":"Open the following link address : 192.168.55.1:8888 The JupyterLab server running on the Jetson Nano will open up with a login prompt the first time. Enter the password: dlinano You will see this screen. Congratulations!","title":"Logging Into The JupyterLab Server"},{"location":"setup/#headless-device-mode-setup-for-jetson-nano-demonstration","text":"","title":"Headless Device Mode Setup For Jetson Nano Demonstration"},{"location":"setup/#troubleshooting","text":"The LED does not light up when the DC barrel jack power supply is connected. Check to be sure you have shorted the two pins on the J48 header with a jumper. The LED lights up, but I cannot access the JupyterLab server from my browser. Try a different browser. I cannot access the JupyterLab server from any browser. Check your computer to see if any new USB devices are recognized when plugging in the USB cable to your computer If on Windows, check ?Device Manager? to see if any new device was added. My computer does not recognize Jetson Nano when connected. Check your USB cable to see if it is enabled for data transfer. You can test it by connecting the Micro-B end of the USB cable to some other USB peripheral such as tablet, Kindle, or other device that communicates over a USB Micro-B port. My USB cable seems good, but my computer does not recognize Jetson Nano. Check if your Jetson Nano Developer Kit is properly booting by connecting it to a TV through an HDMI cable. See if the TV displays the NVIDIA logo when booted, and eventually displays the Ubuntu desktop. My Jetson Nano does not show anything on the TV when booting with the TV attached. Check if you have inserted your microSD card all the way into the microSD card slot. You should hear a small click sound. The microSD card is fully inserted, but my Jetson Nano does not boot properly. Go back to ?Write Image To The MicroSD Card? to reflash your SD card.","title":"Troubleshooting"},{"location":"setup/#-_3","text":"","title":"----"},{"location":"setup/#camera-setup","text":"Now that you've verified that your system can boot to your Jetson Nano Developer Kit with the microSD card, let's add the camera! Power down the Jetson Nano Developer Kit by unplugging the power. Then connect the camera using the instructions below.","title":"Camera Setup"},{"location":"setup/#connecting-the-logitech-c270-webcam-recommended-configuration","text":"This is very straightforward. Just plug in the USB connector into any of the Jetson Nano Developer Kit USB ports.","title":"Connecting The Logitech C270 Webcam (Recommended Configuration)"},{"location":"setup/#alternate-configuration-raspberry-pi-v2-camera","text":"If you want to try this lab with a RasBerry Pi v2 Camera, you will need to connect to the MIPI CSI port. Begin by unlatching the MIPI CSI connector. This loosens the \"grip\" of the connector by just a small amount. Insert the ribbon cable of the camera so that the metal side faces into the Nano board. Latch the connector with a gentle push downward on the sides of the plastic. The ribbon cable should be securely held by the connector Remove the protective film from the lens of the camera.","title":"(Alternate Configuration) Raspberry Pi V2 Camera"},{"location":"setup/#other-cameras","text":"Other cameras may also work with your Jetson Nano Developer Kit. You'll need to test them to find out. If you have another camera on hand, such as a USB webcam, feel free to give it a try using the \"Hello Camera\" test notebooks in the next lesson.","title":"Other Cameras"},{"location":"setup/#raspberry-pi-camera-connection-demonstration","text":"","title":"Raspberry Pi Camera Connection Demonstration"},{"location":"setup/#troubleshooting_1","text":"The camera and JupyterLab appear \"frozen\" If using a Raspberry Pi Camera Module v2, check to ensure it does not touch any of the metal parts (headers, pads, t erminals, ports) of the Jetson Nano Developer Kit board as this may cause an electrical short. JupyterLab is working, but I cannot execute a cell to run my camera. I get an error or it \"hangs\" The camera may have previously been assigned but not released. Shutdown the kernel for the notebook using the pulldown menu at the top of JupyterLab. Open a terminal window from the Launch page (if no Launch page, click the '+' icon) enter sudo systemctl restart nvargus-daemon in the terminal window. You will be prompted for the password, which is dlinano Restart your notebook","title":"Troubleshooting"},{"location":"setup/#-_4","text":"","title":"----"},{"location":"setup/#hello-camera","text":"Now with the camera attached, boot the system using headless Device Mode as you did before.","title":"Hello Camera"},{"location":"setup/#boot-with-camera-attached","text":"Disconnect the USB cable from the computer if it is still attached Connect your DC barrel jack power supply (5V/4A). The Jetson Nano Developer Kit will power on and boot automatically. Wait about 30 seconds. Then connect the USB cable from the Micro USB port on the Jetson Nano Developer Kit to the USB port on your computer. Open a browser window on your computer and enter the address to the Jetson Nano JupyterLab server: 192.168.55.1:8888 The JupyterLab server running on the Jetson Nano will open up.","title":"Boot With Camera Attached"},{"location":"setup/#open-the-hello-camera-notebook","text":"The JupyterLab interface is a dashboard that provides access to the Jupyter interactive notebooks and the operating system for Jetson Nano. The first view you'll see includes a directory tree on the left and a \"Launcher\" page on the right. To open the \"Hello Camera\" notebook: Navigate to the nvdli-nano folder with a double-click Navigate to the hello_camera folder in the same way If you are testing a USB webcam camera such as the Logitech C270, double-click the usb_camera.ipynb notebook to open it. If you are testing a CSI camera, double-click the csi_camera.ipynb instead. Find out more about JupyterLab in the next section. If you are already familiar with JupyterLab features, go ahead and jump right in! When you're satisfied that your camera works correctly, return here for project instructions.","title":"Open The Hello Camera Notebook"},{"location":"setup/#-_5","text":"","title":"----"},{"location":"setup/#jupyterlab","text":"For this course, your Jetson Nano has been configured to run a JupyterLab server on port 8888. When you boot the system and open a browser to the Jetson Nano IP address at that port, you see the JupyterLab interface.","title":"JupyterLab"},{"location":"setup/#jupyterlab-interface","text":"The JupyterLab Interface is a dashboard that provides access to interactive iPython notebooks, as well as the folder structure for your Jetson Nano and a terminal window into the Ubuntu operating system. The first view you'll see includes a menu bar at the top, a directory tree in the left sidebar, and a main work area that is initially open to the \"Launcher\" page. Complete details for all the features and menu actions available can be found in the JupyterLab Interface document. Here are some key capabilities that will be especially useful in this course: File browser: The file browser in the left sidebar allows navigation through the Jetson Nano file structure. Double-clicking on a notebook or file opens it in the main work area. iPython notebooks: The interactive notebooks used in this course have an \".ipynb\" file extension. When a notebook is double-clicked from the file browser, it will open in the main work area and its process will start. The notebooks consist of text and code \"cells\". When a code cell is \"run\", by clicking the run button at the top of the notebook or the keyboard shortcut [CTRL][ENTER], the block of code in the cell is executed and the output, if there is any, appears below the cell in the notebook. To the left of each executable cell there is an \"execution count\" or \"prompt number\" in brackets. If the cell takes more than a few seconds to run, you will see an asterisk mark there, indicating that the cell has not finished its execution. Once processing of that cell is finished, a number will show in the brackets. Kernel operations: The kernel for each running notebook is a separate process that runs the user code. The kernel starts automatically when the notebook is opened from the file browser. The kernel menu on the main menu bar includes commands to shutdown or restart the kernel, which you will need to use periodically. After a kernel shutdown, no code cells can be executed. When a kernel is restarted, all memory is lost regarding imported packages, variable assignments, and so on. Cell tabs: You can move any cell to new window tabs in the main work area by right-clicking the cell and selecting \"Create New View for Output\". This way, you can continue to scroll down the JupyterLab notebook while still watching a particular cell. This is especially helpful in the cell includes a camera view! Terminal window: You can work directly in a Terminal window on your Jetson Nano Ubuntu OS. From the Launcher page, click the Terminal icon under \"Other\". To bring up the Launcher page, if it is no longer visible, click the \"+\" icon at the top of the left sidebar.","title":"JupyterLab Interface"},{"location":"tabel/","text":"Home Getting Started Setup AI and Deep Learning Image Classification Thumb Project Emotion Image Regression Face XY Tutorial Niblack Binarization CUDA OpenCV Tips n Tricks Troubleshoots About","title":"Tabel"}]}