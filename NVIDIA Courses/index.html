



<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-5.5.13">
    
    
      
        <title>Deep Learning Tutorial - Febby@Newtonbau</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.077507d7.min.css">
      
      
    
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#newtonbau-jetson-nano-_-getting-started" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href=".." title="Febby@Newtonbau" class="md-header-nav__button md-logo" aria-label="Febby@Newtonbau">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      
        <div class="md-header-nav__ellipsis">
          <span class="md-header-nav__topic md-ellipsis">
            Febby@Newtonbau
          </span>
          <span class="md-header-nav__topic md-ellipsis">
            
              Deep Learning Tutorial
            
          </span>
        </div>
      
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active">
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
        
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Febby@Newtonbau" class="md-nav__button md-logo" aria-label="Febby@Newtonbau">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    Febby@Newtonbau
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Home" class="md-nav__link">
      Home
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-2" type="checkbox" id="nav-2">
    
    <label class="md-nav__link" for="nav-2">
      Getting Started
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Getting Started" data-md-level="1">
      <label class="md-nav__title" for="nav-2">
        <span class="md-nav__icon md-icon"></span>
        Getting Started
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../setup/" title="Setting up the Jetson Nano" class="md-nav__link">
      Setting up the Jetson Nano
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="./" title="Image Classification" class="md-nav__link">
      Image Classification
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../imageregression/" title="Image Regression" class="md-nav__link">
      Image Regression
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-3" type="checkbox" id="nav-3">
    
    <label class="md-nav__link" for="nav-3">
      Example Project
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="Example Project" data-md-level="1">
      <label class="md-nav__title" for="nav-3">
        <span class="md-nav__icon md-icon"></span>
        Example Project
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../binarization1/" title="Binarization" class="md-nav__link">
      Binarization
    </a>
  </li>

        
          
          
          


  <li class="md-nav__item">
    <a href="../Niblack%20Binarization/" title="Niblack Binarization" class="md-nav__link">
      Niblack Binarization
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        Deep Learning Tutorial
        <span class="md-nav__icon md-icon"></span>
      </label>
    
    <a href="./" title="Deep Learning Tutorial" class="md-nav__link md-nav__link--active">
      Deep Learning Tutorial
    </a>
    
      
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    Prerequisites
  </a>
  
    <nav class="md-nav" aria-label="Prerequisites">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    Hardware
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computer" class="md-nav__link">
    Computer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ai-and-deep-learning" class="md-nav__link">
    AI and Deep Learning
  </a>
  
    <nav class="md-nav" aria-label="AI and Deep Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deep-learning-models" class="md-nav__link">
    Deep Learning Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-neural-networks-cnns" class="md-nav__link">
    Convolutional Neural Networks (CNNs)
  </a>
  
    <nav class="md-nav" aria-label="Convolutional Neural Networks (CNNs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#artificial-neural-network" class="md-nav__link">
    Artificial Neural Network
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convolutions" class="md-nav__link">
    Convolutions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accelerating-cnns-using-gpus" class="md-nav__link">
    Accelerating CNNs Using GPUs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resnet-18" class="md-nav__link">
    ResNet-18
  </a>
  
    <nav class="md-nav" aria-label="ResNet-18">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#residual-networks" class="md-nav__link">
    Residual Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transfer-learning" class="md-nav__link">
    Transfer Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thumbs-project" class="md-nav__link">
    Thumbs Project
  </a>
  
    <nav class="md-nav" aria-label="Thumbs Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interactive-tool-startup-steps" class="md-nav__link">
    Interactive Tool Startup Steps
  </a>
  
    <nav class="md-nav" aria-label="Interactive Tool Startup Steps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-open-the-notebook" class="md-nav__link">
    Step 1: Open The Notebook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-execute-all-of-the-code-blocks" class="md-nav__link">
    Step 2: Execute All Of The Code Blocks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-collect-your-initial-data" class="md-nav__link">
    Step 3: Collect Your Initial Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-train-your-initial-data" class="md-nav__link">
    Step 4: Train Your Initial Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-test-your-data-in-real-time" class="md-nav__link">
    Step 5: Test Your Data In Real Time
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-improve-your-model" class="md-nav__link">
    Step 6: Improve Your Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-7-save-your-model" class="md-nav__link">
    Step 7: Save Your Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#emotions-project" class="md-nav__link">
    Emotions Project
  </a>
  
    <nav class="md-nav" aria-label="Emotions Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interactive-tool-startup-steps_1" class="md-nav__link">
    Interactive Tool Startup Steps
  </a>
  
    <nav class="md-nav" aria-label="Interactive Tool Startup Steps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-open-the-notebook_1" class="md-nav__link">
    Step 1: Open The Notebook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-modify-the-task-code-cell" class="md-nav__link">
    Step 2: Modify The Task Code Cell
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-execute-all-of-the-code-blocks" class="md-nav__link">
    Step 3: Execute All Of The Code Blocks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-collect-data-train-test" class="md-nav__link">
    Step 4: Collect Data, Train, Test
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-improve-your-model" class="md-nav__link">
    Step 5: Improve Your Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-save-your-model" class="md-nav__link">
    Step 6: Save Your Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more-classification-projects" class="md-nav__link">
    More Classification Projects
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-example-on-jetson-nano" class="md-nav__link">
    Running example on Jetson Nano
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-visionworks-sfm-sample" class="md-nav__link">
    Running VisionWorks - SFM Sample
  </a>
  
</li>
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../_______________/" title="___________________" class="md-nav__link">
      ___________________
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../CUDA%20Programming/" title="CUDA" class="md-nav__link">
      CUDA
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../OpenCV%20Programming/" title="OpenCV" class="md-nav__link">
      OpenCV
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../Setup%20and%20Troubleshoots/" title="Setup & Troubleshoot" class="md-nav__link">
      Setup & Troubleshoot
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../Tips%20n%20Tricks/" title="Tips & Tricks" class="md-nav__link">
      Tips & Tricks
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item md-nav__item--nested">
    
      <input class="md-nav__toggle md-toggle" data-md-toggle="nav-10" type="checkbox" id="nav-10">
    
    <label class="md-nav__link" for="nav-10">
      About
      <span class="md-nav__icon md-icon"></span>
    </label>
    <nav class="md-nav" aria-label="About" data-md-level="1">
      <label class="md-nav__title" for="nav-10">
        <span class="md-nav__icon md-icon"></span>
        About
      </label>
      <ul class="md-nav__list" data-md-scrollfix>
        
        
          
          
          


  <li class="md-nav__item">
    <a href="../CV/" title="CV" class="md-nav__link">
      CV
    </a>
  </li>

        
      </ul>
    </nav>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#installation" class="md-nav__link">
    Installation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#prerequisites" class="md-nav__link">
    Prerequisites
  </a>
  
    <nav class="md-nav" aria-label="Prerequisites">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#hardware" class="md-nav__link">
    Hardware
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#computer" class="md-nav__link">
    Computer
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ai-and-deep-learning" class="md-nav__link">
    AI and Deep Learning
  </a>
  
    <nav class="md-nav" aria-label="AI and Deep Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#deep-learning-models" class="md-nav__link">
    Deep Learning Models
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-neural-networks-cnns" class="md-nav__link">
    Convolutional Neural Networks (CNNs)
  </a>
  
    <nav class="md-nav" aria-label="Convolutional Neural Networks (CNNs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#artificial-neural-network" class="md-nav__link">
    Artificial Neural Network
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convolutions" class="md-nav__link">
    Convolutions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accelerating-cnns-using-gpus" class="md-nav__link">
    Accelerating CNNs Using GPUs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resnet-18" class="md-nav__link">
    ResNet-18
  </a>
  
    <nav class="md-nav" aria-label="ResNet-18">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#residual-networks" class="md-nav__link">
    Residual Networks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transfer-learning" class="md-nav__link">
    Transfer Learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#thumbs-project" class="md-nav__link">
    Thumbs Project
  </a>
  
    <nav class="md-nav" aria-label="Thumbs Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interactive-tool-startup-steps" class="md-nav__link">
    Interactive Tool Startup Steps
  </a>
  
    <nav class="md-nav" aria-label="Interactive Tool Startup Steps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-open-the-notebook" class="md-nav__link">
    Step 1: Open The Notebook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-execute-all-of-the-code-blocks" class="md-nav__link">
    Step 2: Execute All Of The Code Blocks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-collect-your-initial-data" class="md-nav__link">
    Step 3: Collect Your Initial Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-train-your-initial-data" class="md-nav__link">
    Step 4: Train Your Initial Data
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-test-your-data-in-real-time" class="md-nav__link">
    Step 5: Test Your Data In Real Time
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-improve-your-model" class="md-nav__link">
    Step 6: Improve Your Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-7-save-your-model" class="md-nav__link">
    Step 7: Save Your Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    |
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#emotions-project" class="md-nav__link">
    Emotions Project
  </a>
  
    <nav class="md-nav" aria-label="Emotions Project">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#interactive-tool-startup-steps_1" class="md-nav__link">
    Interactive Tool Startup Steps
  </a>
  
    <nav class="md-nav" aria-label="Interactive Tool Startup Steps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-1-open-the-notebook_1" class="md-nav__link">
    Step 1: Open The Notebook
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-2-modify-the-task-code-cell" class="md-nav__link">
    Step 2: Modify The Task Code Cell
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-3-execute-all-of-the-code-blocks" class="md-nav__link">
    Step 3: Execute All Of The Code Blocks
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-4-collect-data-train-test" class="md-nav__link">
    Step 4: Collect Data, Train, Test
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-5-improve-your-model" class="md-nav__link">
    Step 5: Improve Your Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#step-6-save-your-model" class="md-nav__link">
    Step 6: Save Your Model
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#more-classification-projects" class="md-nav__link">
    More Classification Projects
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-example-on-jetson-nano" class="md-nav__link">
    Running example on Jetson Nano
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#running-visionworks-sfm-sample" class="md-nav__link">
    Running VisionWorks - SFM Sample
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                  
                
                
                <h1 id="newtonbau-jetson-nano-_-getting-started">Newtonbau : Jetson Nano _ Getting Started</h1>
<p>The following tutorial is a combination of courses organized by NVIDIA, to more detail, you can follow these courses, (at the time of writing these courses are still free), in this section I will summarize 2 couurses at once, who knows in the future there will be changes and not free again, also to simplify and speed up the learning process
edited: There are two version of free course 'Getting Started with Jetson Nano', in this tutorial i will summarize the version.1 and startin from 5th Oktober 2020, the second version is available trough this link.</p>
<p>version 1 will be not available after 5th April 2021
https://courses.nvidia.com/courses/course-v1:DLI+C-RX-02+V1/info</p>
<h2 id="introduction">introduction</h2>
<h2 id="installation">Installation</h2>
<p>Please download the image for Jetson Nano from the  <a href="https://developer.download.nvidia.com/training/nano/ainano_v1-1-1_20GB_200203B.zip">following link</a></p>
<h2 id="prerequisites">Prerequisites</h2>
<p>In order to be successful in this tutorial, you will need the following:</p>
<h3 id="hardware">Hardware</h3>
<ul>
<li>Jetson Nano Developer Kit</li>
<li>Computer with Internet Access and SD card port</li>
<li>microSD Memory Card (32GB UHS-I minimum)</li>
<li>compatible 5V 4A Power Supply with 2.1mm DC barrel connector</li>
<li>2-pin jumper</li>
<li>USB cable (Micro-B to Type-A)</li>
<li>compatible USB webcam camera such as</li>
<li>Logitech C270 Webcam</li>
</ul>
<p>The complete hardware kit is available from Sparkfun either with the Jetson Nano included or without the Jetson Nano included.</p>
<h3 id="computer">Computer</h3>
<ul>
<li>A computer with an internet connection and the ability to flash your microSD card</li>
<li>An available USBA port on your computer (you may need an adapter if you only have USBC ports</li>
</ul>
<h2 id="_1">|</h2>
<hr />
<h2 id="ai-and-deep-learning">AI and Deep Learning</h2>
<p>In this tutorial, you'll build AI projects that can answer simple visual questions:</p>
<ul>
<li>Is my hand showing thumbs-up or thumbs-down?</li>
<li>Does my face appear happy or sad?</li>
<li>How many fingers am I holding up?</li>
<li>Where's my nose?</li>
</ul>
<p>Although these questions are easy for any human child to answer, interpreting images with computer vision requires a complex computer model that can be tuned to find the answer in a number of scenarios. For example, a thumbs-up hand signal may be at various angles and distances from the camera, it may be held before a variety of backgrounds, it could be from a variety of different hands, and so on, but it is still a thumbs-up hand signal. An effective AI model must be able to generalize across these scenarios, and even predict the correct answer with new data.</p>
<p>As humans, we generalize what we see based on our experience. In a similar way, we can use a branch of AI called Machine Learning to generalize and classify images based on experience in the form of lots of example data. In particular, we will use deep neural network models, or Deep Learning to recognize relevant patterns in an image dataset, and ultimately match new images to correct answers.</p>
<p><img alt="AI-ML-DL" src="../img/ai-ml-dl.png" /></p>
<p>If you want to know more, you can check out this article about the differences between Artificial Intelligence, Machine Learning, and Deep Learning.</p>
<h3 id="deep-learning-models">Deep Learning Models</h3>
<p>A Deep Learning model consists of a neural network with internal parameters, or weights, configured to map inputs to outputs. In Image Classification, the inputs are the pixels from a camera image and the outputs are the possible categories, or classes that the model is trained to recognize. The choices might be 1000 different objects, or only two. Multiple labeled examples must be provided to the model over and over to train it to recognize the images. Once the model is trained, it can be run on live data and provide results in real time. This is called inference.</p>
<p><img alt="ai_difference_between_deep_learning_training_inference.jpg" src="../img/ai_difference_between_deep_learning_training_inference.jpg" /></p>
<p>Before training, the model cannot accurately determine the correct class from an image input, because the weights are wrong. Labeled examples of images are iteratively submitted to the network with a learning algorithm. If the network gets the "wrong" answer (the label doesn't match), the learning algorithm adjusts the weights a little bit. Over many computationally intensive iterations, the accuracy improves to the point that the model can reliably determine the class for an input image.</p>
<p>As you will discover, the data that is input is one of the keys to a good model, i.e. one that generalizes well regardless of the background, angle, or other "noisy" aspect of the image presented. Additional passes through the data set, or epochs can also improve the model's performance.</p>
<h2 id="_2">|</h2>
<hr />
<h2 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h2>
<p>Deep learning relies on Convolutional Neural Network (CNN) models to transform images into predicted classifications. A CNN is a class of artificial neural network that uses convolutional layers to filter inputs for useful information, and is the preferred network for image applications</p>
<h3 id="artificial-neural-network">Artificial Neural Network</h3>
<p>An artificial neural network is a biologically inspired computational model that is patterned after the network of neurons present in the human brain. At each layer, the network transforms input data by applying a nonlinear function to a weighted sum of the inputs. The intermediate outputs of one layer, called features, are used as the input into the next layer. The neural network, through repeated transformations, learns multiple layers of nonlinear features (like edges and shapes), which it then combines in a final layer to create a prediction (of more complex objects).</p>
<p><img alt="" src="../img/artificial_neural_network.png" /></p>
<h3 id="convolutions">Convolutions</h3>
<p>The convolution operation specific to CNNs combines the input data (feature map) from one layer with a convolution kernel (filter) to form a transformed feature map for the next layer. CNNs for image classification are generally composed of an input layer (the image), a series of hidden layers for feature extraction (the convolutions), and a fully connected output layer (the classification).</p>
<p><img alt="" src="../img/convolutional_neural_network.png" /> </p>
<p>An input image of a traffic sign is filtered by 4 5x5 convolutional kernels which create 4 feature maps, these feature maps are subsampled by max pooling. The next layer applies 10 5x5 convolutional kernels to these subsampled images and again we pool the feature maps. The final layer is a fully connected layer where all generated features are combined and used in the classifier (essentially logistic regression). Image by Maurice Peemen.
As it is trained, the CNN adjusts automatically to find the most relevant features based on its classification requirements. For example, a CNN would filter information about the shape of an object when confronted with a general object recognition task but would extract the color of the bird when faced with a bird recognition task. This is based on the CNN's recognition through training that different classes of objects have different shapes but that different types of birds are more likely to differ in color than in shape.</p>
<h3 id="accelerating-cnns-using-gpus">Accelerating CNNs Using GPUs</h3>
<p>The extensive calculations required for training CNN models and running inference through trained CNN models can be quite large in number, requiring intensive compute resources and time. Deep learning frameworks such as Caffe, TensorFlow, and PyTorch, are optimized to run faster on GPUs. The frameworks take advantage of the parallel processing capabilities of a GPU if it is present, speeding up training and inference tasks.</p>
<p><img alt="" src="../img/dlsdk_cuda_frameworks.png" /></p>
<p>The Jetson Nano includes a 128-core NVIDIA Maxwell GPU. Since it can run the full training frameworks, it is also able to re-train networks with transfer learning, a capability you will use in the projects for this tutorial. Jetson Nano enables you to experiment with deep learning and AI on a low-cost platform. See this article for more details on Jetson Nano performance.</p>
<h2 id="_3">|</h2>
<hr />
<h2 id="resnet-18">ResNet-18</h2>
<p>There are a number of world-class CNN architectures available to application developers for image classification and image regression. PyTorch and other frameworks include access to pretrained models from past winners of the famous Imagenet Large Scale Visual Recognition Challenge (ILSVRC), where researchers compete to correctly classify and detect objects and scenes with computer vision algorithms. In 2015, ResNet swept the awards in image classification, detection, and localization. We'll be using the smallest version of ResNet in our projects: ResNet-18.</p>
<p><img alt="" src="../img/resnet18_3d.png" /></p>
<h3 id="residual-networks">Residual Networks</h3>
<p>The Deep Residual Learning for Image Recognition research paper provides insight into why this architecture is effective. ResNet is a residual network, made with building blocks that incorporate "shortcut connections" that skip one or more layers.</p>
<p><img alt="" src="../img/resnet-18_residual.png" /></p>
<p>The shortcut output is added to the outputs of the skipped layers. The authors demonstrate that this technique makes the network easier to optimize, and have higher accuracy gains at greatly increased depths. The ResNet architectures presented range from 18-layers deep, all the way to 152-layers deep! For our purposes, the smallest network, ResNet-18 provides a good balance of performance and efficiency sized well for the Jetson Nano.</p>
<h3 id="transfer-learning">Transfer Learning</h3>
<p>PyTorch includes a pre-trained ResNet-18 model that was trained on the ImageNet 2012 classification dataset, which consists of 1000 classes. In other words, the model can recognize 1000 different objects already!</p>
<p>Within the trained neural network are layers that find outlines, curves, lines, and other identifying features of an image. Important image features that were already learned in the original training of the model are now re-usable for our own classification task.</p>
<p><img alt="" src="../img/autos-672x378.png" /></p>
<p>We will adapt it for our projects, which all include less than 10 different classes, by modifying the last neural network layer of the 18 that make up the ResNet-18 model. The last layer for ResNet-18 is a fully connected (fc) layer, pooled and flattened to 512 inputs, each connected to the 1000 possible output classes. We will replace the (512,1000) layer with one matching our classes. If we only need three classes, for example, this final layer will become (512, 3), where each of the 512 inputs is fully connected to each one of the 3 output classes.</p>
<p>You will still need to train the network to recognize those three classes using images you collect, but since the network has already learned to recognize features common to most objects, training is already part-way done. The previous training can be reused, or "transferred" to your new projects.</p>
<h2 id="_4">|</h2>
<hr />
<h2 id="thumbs-project">Thumbs Project</h2>
<p>The goal of this exercise is to build an Image Classification project that can determine the meaning of hand signals ( thumbs-up or thumbs-down) that are held in front of a live camera.
<img alt="" src="../img/thumbs_2classes.png" /></p>
<h3 id="interactive-tool-startup-steps">Interactive Tool Startup Steps</h3>
<p>You will implement the project by collecting your own data, training a model to classify your data, and then testing and updating your model as needed until it correctly classifies thumbs-up or thumbs-down images before the live camera.</p>
<h4 id="step-1-open-the-notebook">Step 1: Open The Notebook</h4>
<p>To get started, navigate to the classification folder in your JupyterLab interface and double-click the <strong>classification_interactive.ipynb</strong> notebook to open it.</p>
<h4 id="step-2-execute-all-of-the-code-blocks">Step 2: Execute All Of The Code Blocks</h4>
<p>The notebook is designed to be reusable for any classification task you wish to build. Step through the code blocks and execute them one at a time. If you have trouble with this step, review the information on JupyterLab.</p>
<ol>
<li>
<p>Camera
This block sets the size of the images and starts the camera. If your camera is already active in this notebook or in another notebook, first shut down the kernel in the active notebook before running this code cell. Make sure that the correct camera type is selected for execution (USB). This cell may take several seconds to execute.</p>
</li>
<li>
<p>Task
You get to define your <strong>TASK</strong> and <strong>CATEGORIES</strong> (the classes) parameters here, as well as how many datasets you want to track. For the Thumbs Project, this has already been defined for you, so go ahead and execute the cell. Subdirectories for each class are created to store the example images you collect. The subdirectory names serve as the labels needed for the model. This cell should only take a few seconds to execute.</p>
</li>
<li>
<p>Data Collection
You'll collect images for your categories with your camera using an iPython widget. This cell sets up the collection mechanism to count your images and produce the user interface. The widget built here is the data_collection_widget. If you want to learn more about these powerful tools, visit the ipywidgets documentaion. This cell should only take a few seconds to execute.</p>
</li>
<li>
<p>Model
This block is where the neural network is defined. First, the GPU device is chosen with the statement:</p>
<p>device = torch.device('cuda')</p>
</li>
</ol>
<p>The model is set to the ResNet-18 model for this project. Note that the pretrained=True parameter indicates we are loading all the parameter weights for the trained Resnet-18 model, not just the neural network alone:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>model = torchvision.models.resnet18(pretrained=True)
</code></pre></div>
</td></tr></table>

<p>There are a few more models listed in comments that you can try out later if you wish. For more information on available PyTorch pre-trained models, see the PyTorch documentation.</p>
<p>In addition to choosing the model, the last layer of the model is modified to accept only the number of classes that we are training for. In the case of the Thumbs Project, it is only 2 (i.e. thumbs-up and thumbs-down).</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code>model.fc = torch.nn.Linear(512, len(dataset.categories))
</code></pre></div>
</td></tr></table>

<p>This code cell may take several seconds to execute.</p>
<ol>
<li>
<p>Live Execution
This code block sets up threading to run the model in the background so that you can view the live camera feed and visualize the model performance in real time. It also includes the code that defines how the outputs from the neural network are categorized. The network produces some value for each of the possible categories. The softmax function takes this vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities. The values now add up to 1 and can be interpreted as probabilities.</p>
<p>output = F.softmax(output, dim=1).detach().cpu().numpy().flatten()
This cell should only take a few seconds to execute.</p>
</li>
<li>
<p>Training and Evaluation
The training code cell sets the hyper-parameters for the model training (number of epochs, batch size, learning rate, momentum) and loads the images for training or evaluation. The model determines a predicted output from the loaded input image. The difference between the predicted output and the actual label is used to calculate the "loss". If the model is in training mode, the loss is backpropagated into the network to improve the model. The widgets created by this code cell include the option for setting the number of epochs to run. One epoch is a complete cycle of all images through the trainer. This code cell may take several seconds to execute.</p>
</li>
<li>
<p>Display the Interactive Tool!
This is the last code cell. All that's left to do is pack all the widgets into one comprehensive tool and display it. This cell may take several seconds to run and should display the full tool for you to work with. This tool will look essentially the same, no matter how you set up the classification problem with this notebook.</p>
</li>
</ol>
<p><img alt="" src="../img/classification_interactive_tool.png" /></p>
<h4 id="step-3-collect-your-initial-data">Step 3: Collect Your Initial Data</h4>
<p>The tool is designed for live interaction, so you can collect some data, train it, check the results, and then improve the model with more data and training. We'll try this in pieces to learn what effect the data you gather has on performance of the model. At each step, you'll vary the data in a new way, building your dataset as you go.</p>
<p>Collect 30 images of thumbs-up images. Move your thumb through an arc of generally upward angles in front of the camera as you click the "add" button to save the data images.
<img alt="" src="../img/rotate_thumb.png" />
Next, select the thumbs-down category on the tool
<img alt="" src="../img/thumbsdown_datacollection_select.png" />
and collect 30 images of your thumb in the down position, again varying the angle a bit as you click. The goal is to provide the model with lots of different examples from each category, so that the prediction can be generalized.</p>
<h4 id="step-4-train-your-initial-data">Step 4: Train Your Initial Data</h4>
<p>Set the epoch number to 10, and click the train button. There will be a delay of about 30 seconds as the trainer loads the data. After that, the progress bar will indicate training status for each epoch. You'll see the calculated loss and accuracy displayed as well. With each epoch, the model improves, at least based on the data it has to work with! The accuracy should generally increase. Keep in mind that the accuracy is based on tests against the data the model already has access to, not truly unseen data.</p>
<p><img alt="" src="../img/thumbs_train_closeup.png" /></p>
<h4 id="step-5-test-your-data-in-real-time">Step 5: Test Your Data In Real Time</h4>
<p>Once training is done, hold your thumb up or down in front of the camera and observe the prediction and sliders. The sliders indicate the probability the model gives for the prediction made. How was the result? Are you satisfied that the model is robust? Try moving the camera to a new background to see if it still works the same.</p>
<p><strong>Note</strong>: If at any time your camera seems to "freeze", it will be necessary to shut down the kernel from the menu bar, then restart the kernel and run all cells. Your data is saved, but the model training will need to be run again</p>
<p><img alt="" src="../img/thumbsup_prediction1.png" /></p>
<h4 id="step-6-improve-your-model">Step 6: Improve Your Model</h4>
<ul>
<li>Using a different background, gather an additional 30 images for thumbs-up and thumbs-down, again varying the angle. Train an additional 5 epochs.</li>
<li>Did your model become more reliable? What happens when you move the thumb to corners and edges of the camera view, or move your thumb very far away or very close to the camera?</li>
<li>Using a variety of distances from the camera, gather an additional 30 images for thumbs-up and thumbs-down. Train an additional 5 epochs.</li>
<li>Keep testing and training in this way until you are satisfied with the performance of your first project!</li>
</ul>
<h4 id="step-7-save-your-model">Step 7: Save Your Model</h4>
<p>When you are satisfied with your model, save it by entering a name in the "model path" box and click "save model".</p>
<h2 id="_5">|</h2>
<hr />
<h2 id="emotions-project">Emotions Project</h2>
<p>The goal of this exercise is to build an Image Classification project that can determine the meaning of four different facial expressions ("happy", "sad", "angry", "none"), that you provide in front of a live camera.</p>
<h3 id="interactive-tool-startup-steps_1">Interactive Tool Startup Steps</h3>
<p>The setup for the Emotions Project is almost the as for the Thumbs Project.</p>
<h4 id="step-1-open-the-notebook_1">Step 1: Open The Notebook</h4>
<p>You'll use the same <strong>classification_interactive.ipynb</strong> notebook. If it's already open, restart the notebook and clear all the outputs using the Kernel menu with <strong>Kernel-&gt;Restart Kernel and Clear All Outputs</strong>. If your camera is active in any other notebook, shut down the kernel in that active notebook as well.</p>
<h4 id="step-2-modify-the-task-code-cell">Step 2: Modify The Task Code Cell</h4>
<p>Before you execute all of the code blocks in the notebook, you'll need to change the <strong>TASK</strong> and <strong>CATEGORIES</strong> parameters in the Task code cell block to define the new project. Comment out the "thumbs" project parameters, and uncomment the "emotions" parameters:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4
5
6
7
8
9</pre></div></td><td class="code"><div class="highlight"><pre><span></span><code># TASK = &#39;thumbs&#39;
TASK = &#39;emotions&#39;
# TASK = &#39;fingers&#39;
# TASK = &#39;diy&#39;

# CATEGORIES = [&#39;thumbs_up&#39;, &#39;thumbs_down&#39;]
CATEGORIES = [&#39;none&#39;, &#39;happy&#39;, &#39;sad&#39;, &#39;angry&#39;]
# CATEGORIES = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;]
# CATEGORIES = [ &#39;diy_1&#39;, &#39;diy_2&#39;, &#39;diy_3&#39;]
</code></pre></div>
</td></tr></table>

<h4 id="step-3-execute-all-of-the-code-blocks">Step 3: Execute All Of The Code Blocks</h4>
<p>The rest of the blocks remain the same. You'll still use the ResNet18 pre-trained model as a base. This time, since there are four items in the CATEGORIES parameter, there will be four different class subdirectories for data and four output probability sliders on the Interactive tool.</p>
<p><img alt="" src="../img/emotions_interactive.png" /></p>
<h4 id="step-4-collect-data-train-test">Step 4: Collect Data, Train, Test</h4>
<p>Position the camera in front of your face and collect initial data. As you collect each emotion, vary your head position and pose. Try leaning your head left and right, up and down, side to side. As you create your emotion faces, think about the difference between sad and angry. Exaggerate Your expressions to make them distinctive for the initial training, then refine with more subtlety as you as your model improves:</p>
<ol>
<li>Add 20 images of a "happy" face with the happy category selected</li>
<li>Add 20 images of a "sad" face with the sad category selected</li>
<li>Add 20 images of an "angry" face with the angry category selected</li>
<li>Add 20 images of a face with no expression with the none category selected</li>
<li>Set the number of epochs to 10 and click the train button</li>
<li>Once the training is complete, try different expressions live and observe the prediction</li>
</ol>
<h4 id="step-5-improve-your-model">Step 5: Improve Your Model</h4>
<p>As you did in the Thumbs Project, you can improve your model by adding data for scenarios that don't work as well as you like, then retraining. For example:</p>
<ul>
<li>Move the camera so that the face is closer. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training.</li>
<li>Move the camera for a different background. Is the performance of the predictor still good? If not, try adding some data for each category (10 each) and retrain (5 epochs). Does this help? You can experiment with more data and more training.</li>
<li>Can you get a friend to try your model? Does it work the same? You know the drill… more data and training!</li>
</ul>
<h4 id="step-6-save-your-model">Step 6: Save Your Model</h4>
<p>When you are satisfied with your model, save it by entering a name in the "model path" box and click "save model".</p>
<h3 id="more-classification-projects">More Classification Projects</h3>
<p>To build another project, follow the pattern you did with the Emotions Project. As an example, the Fingers project is provided as an example, but don't let that limit you! To start a new project, save your previous work, modify the <strong>TASK</strong> and <strong>CATEGORIES</strong> values, shutdown and restart the notebook, and run all the cells. Then collect, train, and test!</p>
<h2 id="running-example-on-jetson-nano">Running example on Jetson Nano</h2>
<h2 id="running-visionworks-sfm-sample">Running VisionWorks - SFM Sample</h2>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../Niblack%20Binarization/" title="Niblack Binarization" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Niblack Binarization
              </div>
            </div>
          </a>
        
        
          <a href="../_______________/" title="___________________" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                ___________________
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
        
        
      
      <a href="https://www.linkedin.com/in/febbymadrin/" target="_blank" rel="noopener" title="www.linkedin.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/vendor.2d1db4bd.min.js"></script>
      <script src="../assets/javascripts/bundle.6627ddf3.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents"}</script>
      
      <script>
        app = initialize({
          base: "..",
          features: ['navigation.tabs'],
          search: Object.assign({
            worker: "../assets/javascripts/worker/search.5eca75d3.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../javascripts/extra.js"></script>
      
    
  </body>
</html>